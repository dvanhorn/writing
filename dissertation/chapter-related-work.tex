Probabilistic languages can be approximately placed into two groups: those defined by an implementation, and those defined by a semantics.

\section{Implementations}

Almost all of the languages defined by their implementations support conditional queries and compute converging approximations.
The reports on these languages generally describe interpreters, compilers, and algorithms for sampling with probabilistic conditions.
When they work correctly, they are useful.

Koller and Pfeffer~\cite{cit:koller-1997aaai-bayes-programs-short} efficiently compute exact, discrete distributions for the outputs of programs in a Scheme-like language.
BUGS~\cite{cit:winbugs-language-short} focuses on efficient approximate computation for probabilistic theories with a finitely many statements, and uses approximation methods that Bayesians typically use.
BLOG~\cite{cit:blog-language-short} exists specifically to allow stating distributions over countably infinite vectors.
BLAISE~\cite{cit:blaise-language} allows stating both distributions and approximation methods for random variables.
Church~\cite{cit:church-language-short} is a Scheme-like probabilistic language with approximate inference, and focuses on expressiveness.

Kiselyov~\cite{cit:kiselyov-2008uai-monolingual} embeds a probabilistic language in O'Caml for efficient computation.
It uses continuations to enumerate or sample random variable values, and has a \texttt{fail} construct for the \textit{complement} of conditioning.
The sampler looks ahead for \texttt{fail} and can handle it efficiently.
This may be justified by commutativity (Thm.~\ref{thm:commutativity}), depending on interaction with other language features.

Recently, Wingate et al~\cite{cit:wingate-2011ais-lightweight,cit:wingate-2011nips-nonstandard} have defined the semantics of \emph{nonstandard interpretations} that enable efficient inference, but do not define the languages.
[XXX: recheck, relate to computation indexing]

\section{Semantics}

Early work in probabilistic language semantics is not motivated by Bayesian concerns, and thus does not address conditioning.
Kozen~\cite{cit:kozen-1979fcs-prob-programs-short} defines the meaning of bounded-space, imperative ``while'' programs as functions from probability measures to probability measures.
Hurd~\cite{cit:hurd-2002thesis} proves properties about programs with binary random choice by encoding programs and portions of measure theory in HOL.
Jones~\cite{cit:jones-1990thesis} develops a domain-theoretic account of probability, and with it defines the probability monad, whose discrete version is a distribution-valued variation of the set or list monad.

Ramsey and Pfeffer~\cite{cit:ramsey-2002popl-stochastic-short} define the probability monad measure-theoretically and implement a language for finite probability.
We do not build on this work because the probability monad does not build a global probability space, making it difficult to reason about conditioning.

Using inverse transform sampling, Park~\cite{cit:park-2008toplas-prob} extends a $\lambda$-calculus with probabilistic choice according to any of a general class of probability measures.
This is the same technique we use in Chapter~\ref{ch:preimage2} to turn uniform probabilistic choice into choice according to other distributions, though preimage computation makes doing so more difficult.

Some recent work in probabilistic language semantics tackles conditioning.
Pfeffer's IBAL~\cite{cit:pfeffer-2007chapter-ibal} is the earliest $\lambda$-calculus with finite probabilistic choice that also defines conditional queries.
Borgstr\"om et al~\cite{cit:borgstrom-2011esop-measure-transformer} develop Fun, a first-order functional language without recursion, extended with probabilistic choice and conditioning.
Its semantics interprets programs as \emph{measure transformers} by transforming expressions into arrow-like combinators.
The implementation generates a decomposition of the probability density represented by the program, if it exists.
Bhat et al~\cite{cit:bhat-2013etaps-densities} replaces Fun's $\mathtt{if}$ with $\mathtt{match}$, and interprets programs more directly as probability density functions by compositionally transforming expressions into an extension of the probability monad.

\section{Techniques}

The forward phase in computing preimages takes a subdomain and returns an overapproximation of the function's range for that subdomain.
This clearly generalizes interval arithmetic~\cite{cit:kearfott-1996eb-interval} to all first-order algebraic types.
We further generalize interval arithmetic by doing it backwards as well as forwards.

Our general approach---creating an exact semantics and deriving an implementable, approximating semantics---is similar to abstract interpretation~\cite{cit:cousot-1977popl-abstract-interpretation}.
Most functional programming researchers would hesitate to call Chapter~\ref{ch:countable-models}'s approximating semantics abstract, however.
While the approximations it computes converge, they are not conservative.

The work in Chapter~\ref{ch:preimage1} is much more in line with typical abstract interpretation: $\meaningof{\cdot}\conv\ppre$ is the concrete semantics, ${\meaningof{\cdot}\conv}\ppre'$ is an abstract semantics, the concrete values are arbitrary sets, and the abstract values are rectangles.
It has many properties common to abstract semantics: it is sound, it labels expressions, the abstract domain is a lattice, and the exact semantics performs infinite computations.

However, it is far from typical in other ways.
It is used to run programs, not for static analysis.
The abstraction boundaries are the conditional branches of completely unrolled, infinite programs, and are not fixed.
There is no Kleene iteration to find a fixed point.
In our abstract semantics, infinite computations are done in a library of \lzfclang-computable combinators, not by a semantic function.
This cleanly separates the syntax from the semantics, and allows us to prove the exact semantics correct mostly by proving simple categorical properties.



\section{Somewhat Related Work}

Any programming language research described by the words ``bijective'' or ``reversible'' might seem to have much in common with ours.
Unfortunately, when we look more closely, we can usually draw only loose analogies and perhaps inspiration.
An example is lenses~\cite{cit:hofmann-2012popl-edit-lenses}, which are transformations from $X$ to $Y$ that can be run forward and backward, in a way that maintains some relationship between $X$ and $Y$.
Usually, a destructive, external process is assumed, so that, for example, a change from $y \in Y$ to $y' \in Y$ induces a corresponding change from $x \in X$ to some $x' \in X$.
When transformations lose information, lenses must satisfy certain behavioral laws.
In our work, no input or output is updated, and preimages are always definable regardless of non-injectivity.

Many multi-paradigm languages~\cite{cit:hanus-2007lp-multi-paradigm}, especially constraint functional languages, bear a strong resemblance to our work.
In fact, it is easy to add a $\mathsf{fail}$ expression to our semantics, or to transform constraints into boolean program outputs.
The most obvious difference is evaluation strategy.
The most important difference is that our interpretation of programs returns \emph{distributions} of constrained outputs, rather than arbitrary single values that meet constraints.
