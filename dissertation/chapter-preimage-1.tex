\newsavebox{\codebox}

\begin{quote}
\textit{I am so in favor of the actual infinite that instead of admitting that Nature abhors it, as is commonly said, I hold that Nature makes frequent use of it everywhere, in order to show more effectively the perfections of its Author.}

\hfill Georg Cantor
\end{quote}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Measure-theoretic probability~\cite{cit:klenke-2006-probability} is widely believed to be able to define every reasonable distribution, including distributions arising from discontinuous transformations and distributions on infinite spaces.
It mainly does this by \emph{assigning probabilities to sets}.
Functions that do so are \keyword{probability measures}.

If a probability measure $P$ assigns probabilities to subsets of $X$ and $g : X \to Y$, then the distribution over subsets of $Y$ is
\begin{equation}
	\Pr[B] \ = \ P~(preimage~g~B)
\end{equation}
where $preimage~g~B\ =\ \setb{a \in domain~g}{g~a \in B}$ is the subset of $X$ for which $g$ yields a value in $B$.
It is well-defined for any $g$ and $B$.

Measure-theoretic probability supports any kind of condition.
If $\Pr[B] > \mathrm{0}$, the probability of $B' \subseteq Y$ given $B \subseteq Y$ is
\begin{equation}
	\Pr[B'\,|\,B]\ =\ \Pr[B' \i B]~{/}~\Pr[B]
\label{eqn:bayes-law-preimage}
\end{equation}
If $\Pr[B] = \mathrm{0}$, conditional probabilities can be calculated as the limit of $\Pr[B'\,|\,B_n]$ for certain positive-probability $B_1 \supseteq B_2 \supseteq B_3 \supseteq \cdots$ whose intersection is $B$~\cite{cit:samuels-1978amm-radnik}.
For example, if $Y = \Re \times \Re$, the distribution of $\pair{x,y} \in Y$ given $x + y = \mathrm{0}$ can be calculated using the descending sequence $B_n = \setb{\pair{x,y} \in Y}{|x + y| < \mathrm{2}^{-n}}$.

Only special families of \keyword{measurable} sets can be assigned probabilities.
Proving measurability, taking limits, and other complications tend to make measure-theoretic probability less attractive, even though it is strictly more powerful.

\subsection{Measure-Theoretic Semantics}

Most purely functional languages allow only nontermination as a side effect, and not probabilistic choice.
Programmers therefore encode probabilistic programs as functions from random sources to outputs.
Monads and other categorical classes such as idioms (i.e. applicative functors) can make doing so easier~\cite{cit:hurd-2002thesis,cit:toronto-2010ifl-bayes}.

It seems this approach should make it easy to interpret probabilistic programs measure-theoretically.
For a probabilistic program $g : X \to Y$, the probability measure on output sets $B \subseteq Y$ should be defined by preimages of $B$ under $g$ and the probability measure on $X$.
Unfortunately, it is difficult to turn this simple-sounding idea into a compositional semantics, for the following reasons.
\begin{enumerate}
	\item Preimages are definable only for functions with observable domains, which excludes lambdas.%
\label{problem:observable-domain}
	\item If subsets of $X$ and $Y$ must be measurable, taking preimages under $g$ must preserve measurability (we say $g$ itself is measurable). Proving the conditions under which this is true is difficult, especially if $g$ may not terminate.%
\label{problem:measurability}
	\item It is difficult to define useful probability measures for arbitrary spaces of measurable functions~\cite{cit:aumann-1961ijm-borel}.%
\label{problem:higher-orderness}
\end{enumerate}
Implementing a language based on such a semantics is complicated because
\begin{enumerate}
	\setcounter{enumi}{3}
	\item Contemporary mathematics is unlike any implementation's host language.%
\label{problem:different-language}
	\item It requires running Turing-equivalent programs backwards, efficiently, on possibly uncountable sets of outputs.%
\label{problem:backward-efficient}
\end{enumerate}

We address~\ref{problem:observable-domain} and~\ref{problem:different-language} by developing our semantics in \lzfclang~\cite{cit:toronto-2012flops-lzfc}, a $\lambda$-calculus with infinite sets, and both extensional and intensional functions.
We address~\ref{problem:backward-efficient} by deriving and implementing a \emph{conservative approximation} of the semantics.

There seems to be no way to simplify difficulty~\ref{problem:measurability}, so we work through a proof of measurability.
The outcome is worth it: all probabilistic programs are measurable, regardless of the inputs on which they do not terminate.
This includes uncomputable programs; for example, those that contain real equality tests and limits.
We believe this result is the first of its kind, and is general enough to apply to almost all past and future work on probabilistic programming languages.
To maintain the flow of this chapter, we put it off until Chapter~\ref{ch:measurability}.

For difficulty~\ref{problem:higher-orderness}, we have discovered that the ``first-orderness'' of arrows~\cite{cit:hughes-2000scp-arrows} is a perfect fit for the ``first-orderness'' of measure theory.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mathversion{sans}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Arrow Solution Overview}

\newcommand{\youarehere}[1]%
{%
\begin{equation}%
\begin{CD}%
X \botto Y   @>\liftmap>>   X \mapto Y   @>\liftpre>>   X \preto Y \\%
@V{\eta_\pbot}VV             @VV{\eta\pmap}V              @VV{\eta\ppre}V\\%
X \pbotto Y  @>>\liftpmap>  X \pmapto Y  @>>\liftppre>  X \ppreto Y%
\end{CD}%
\label{#1}%
\end{equation}%
}

Using arrows, we define an \emph{exact} semantics and an \emph{approximating} semantics.
The exact semantics includes
\begin{itemize}
	\item A semantic function which, like the arrow calculus semantic function~\cite{cit:lindley-2010jfp-arrow-calculus}, transforms first-order programs into the computations of an arbitrary arrow.
	\item Arrows for evaluating expressions in different ways.
\end{itemize}
This commutative diagram describes the relationships among the six arrows used to define the exact semantics:
\youarehere{eqn:roadmap-diagram1}
At the top-left, $X \botto Y$ computations (or ``bottom arrow computations'') are intensional functions that may raise errors (i.e. return $\bot$, which is read ``bottom'').
From bottom arrow computations, the $\liftmap$ combinator produces $X \mapto Y$ computations, which create equivalent extensional functions, or mappings.
From mapping arrow computations, the $\liftpre$ combinator produces $X \preto Y$ computations, which compute preimages.

Instances of arrows in the bottom row are like those in the top row, except they thread an infinite store of random values, and can be constructed to always terminate.

Most of our correctness theorems rely on proofs that every combinator in~\eqref{eqn:roadmap-diagram1} is a homomorphism; for example, that $\liftmap$ distributes over all bottom arrow combinators.

The approximating semantics uses the same semantic function, but its arrows $X \preto' Y$ and $X \ppreto' Y$ compute conservative approximations.
Given a library for representing and operating on rectangular sets, it is directly implementable.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Arrows and First-Order Semantics}

Like monads~\cite{cit:wadler-2001-monads} and idioms~\cite{cit:mcbride-2008jfp-idiom}, arrows~\cite{cit:hughes-2000scp-arrows} thread effects through computations in a way that imposes structure.
But arrow computations are always
\begin{itemize}
	\item Function-like: An arrow computation of type $x \arrow\gen y$ must behave like a corresponding function of type $x \tto y$ (in a sense we explain shortly).
	\item First-order: There is no way to derive a computation $app\gen : \pair{x \arrow\gen y, x} \arrow\gen y$ from the arrow $a$'s minimal definition, so it is not possible for an arrow computation to apply another arrow computation.
\end{itemize}
The first property makes arrows a good fit for a compositional translation from expressions to pure functions that operate on random sources.
The second property makes arrows a good fit for a measure-theoretic semantics in particular, as it is difficult to define useful measurable sets of functions that make $app$'s corresponding function measurable~\cite{cit:aumann-1961ijm-borel}.

\subsection{Alternative Arrow Definitions and Laws}
\label{sec:arrow-definitions}

To make applying measure-theoretic theorems easier, and to simplify interpreting let-calculus expressions as arrow computations, we do not give typical minimal arrow definitions.
For each arrow $a$, instead of $first\gen$, we define $(\arrowpair\gen)$.
This combinator is typically called \keyword{fanout}, but its use will be clearer if we call it \keyword{pairing}.
One way to strengthen an arrow $a$ is to define an additional combinator $left\gen$, which can be used to choose an arrow computation based on the result of another.
Again, we define a different combinator, $\arrowif\gen$ (``if-then-else'').

In a nonstrict $\lambda$-calculus, defining a choice combinator allows writing recursive functions using nothing but arrow combinators and lifted, pure functions.
However, a strict $\lambda$-calculus needs an extra combinator $\arrowlazy$ for deferring conditional branches.
For example, define the \keyword{function arrow} with choice, in which $x \arrow y\ ::=\ x \tto y$:
\begin{equation}
\begin{aligned}
	\arrowarr~f &\ := \ f &&\text{lift}\\
	f_1~\arrowcomp~f_2 &\ := \ \fun{a} f_2~(f_1~a) &&\text{composition}\\
	f_1~\arrowpair~f_2 &\ := \ \fun{a} \pair{f_1~a,f_2,a} &&\text{pairing} \\
	\arrowif~f_1~f_2~f_3 &\ := \ \fun{a} if~(f_1~a)~(f_2~a)~(f_3~a) \hspace{0.5in} &&\text{if-then-else} \\
	\arrowlazy~f &\ :=\ \fun{a} f~0~a &&\text{laziness}
\end{aligned}
\label{eqn:function-arrow}
\end{equation}
and try to define the following recursive function:
\begin{equation}
\begin{aligned}
	&\,halt!on!true : Bool \arrow Bool \hspace{0.5in} \text{(i.e. $halt!on!true : Bool \tto Bool$)}
	\\
	&\begin{aligned}
		halt!on!true &\ := \ \arrowif~(\arrowarr~id)~(\arrowarr~id)~halt!on!true
	\\
		&\ \equiv\ \arrowif~id~id~(\arrowif~(\arrowarr~id)~(\arrowarr~id)~halt!on!true)
	\\
		&\ \equiv\ \arrowif~id~id~(\arrowif~id~id~(\arrowif~(\arrowarr~id)~(\arrowarr~id)~halt!on!true))
	\end{aligned}
\end{aligned}
\end{equation}
In a strict $\lambda$-calculus, the defining expression does not terminate.
But the following is well-defined in \lzfclang, and loops only when applied to $false$:
\begin{equation}
\begin{aligned}
	halt!on!true &\ := \ \arrowif~(\arrowarr~id)~(\arrowarr~id)~(\arrowlazy~\fun{0}{halt!on!true})
	\\
		&\ \equiv\ \arrowif~id~id~(\fun{a} (\fun{0} halt!on!true)~0~a)
	\\
		&\ \equiv\ \fun{a} if~(id~a)~(id~a)~((\fun{a} (\fun{0} halt!on!true)~0~a)~a)
	\\
		&\ \equiv\ \fun{a} if~a~a~((\fun{a} halt!on!true~a)~a)
	\\
		&\ \equiv\ \fun{a} if~a~a~(halt!on!true~a)
\end{aligned}
\end{equation}

All of our arrows are arrows with choice and $\arrowlazy$, so we simply call them arrows.

\begin{definition}[arrow]Let $1 := \set{0}$ (Section~\ref{sec:axiom-of-infinity}). A binary type constructor $(\arrow\gen)$ and
\begin{displaybreaks}
\begin{equation}
\begin{aligned}
	\arrowarr\gen &: (x \tto y) \tto (x \arrow\gen y)
	&&\text{lift}
\\*
	(\arrowcomp\gen) &: (x \arrow\gen y) \tto (y \arrow\gen z) \tto (x \arrow\gen z)
	&&\text{composition}
\\
	(\arrowpair\gen) &: (x \arrow\gen y) \tto (x \arrow\gen z) \tto (x \arrow\gen \pair{y,z})
	&&\text{pairing}
\\
	\arrowif\gen &: (x \arrow\gen Bool) \tto (x \arrow\gen y) \tto (x \arrow\gen y) \tto (x \arrow\gen y)
	\hspace{0.5in}&&\text{if-then-else}
\\*
	\arrowlazy\gen &: (1 \tto (x \arrow\gen y)) \tto (x \arrow\gen y)
	&&\text{laziness}
\label{eqn:arrow-combinators}
\end{aligned}
\end{equation}
\end{displaybreaks}
define an \keyword{arrow} if certain monoid, homomorphism, and structural laws hold.
\end{definition}

The arrow homomorphism laws can be put in terms of more general homomorphism properties that deal with distributing an arrow-to-arrow lift, which we use extensively to prove correctness.

\begin{definition}[arrow homomorphism]
\label{def:arrow-homomorphism}
A function $lift\genb : (x \arrow\gen y) \tto (x \arrow\genb y)$ is an \mykeyword{arrow homomorphism} from arrow $\mathrm{a}$ to arrow $\mathrm{b}$ if the following distributive laws hold for appropriately typed $f$, $f_1$, $f_2$ and $f_3$:
\begin{align}
	lift\genb~(\arrowarr\gen~f) &\ \equiv \ \arrowarr\genb~f
	\label{eqn:lift-distributes-over-arr}
\\
	lift\genb~(f_1~\arrowcomp\gen~f_2) &\ \equiv \ (lift\genb~f_1)~\arrowcomp\genb~(lift\genb~f_2)
	\label{eqn:lift-distributes-over-comp}
\\
	lift\genb~(f_1~\arrowpair\gen~f_2) &\ \equiv \ (lift\genb~f_1)~\arrowpair\genb~(lift\genb~f_2)
	\label{eqn:lift-distributes-over-pair}
\\
	\arrowlift\genb~(\arrowif\gen~f_1~f_2~f_3) &\ \equiv \ 
		\arrowif\genb~(lift\genb~f_1)~(lift\genb~f_2)~(lift\genb~f_3)
	\label{eqn:lift-distributes-over-if}
\\
	\arrowlift\genb~(\arrowlazy\gen~f) &\ \equiv \
		\arrowlazy\genb~\fun{0}{\arrowlift\genb~(f~0)}
	\label{eqn:lift-distributes-over-lazy}
\end{align}
\end{definition}

The arrow homomorphism laws state that $\arrowarr\gen : (x \tto y) \tto (x \arrow\gen y)$ must be a homomorphism from the function arrow~\eqref{eqn:function-arrow} to arrow $a$.
Roughly, arrow computations that do not use additional combinators can be transformed into $\arrowarr\gen$ applied to a pure computation.
They must be \emph{function-like}.

Only a few of the other arrow laws play a role in our semantics and its correctness.
We need associativity of $(\arrowcomp\gen)$ and a pair extraction law:
\begin{displaybreaks}
\begin{align}
	(f_1~\arrowcomp\gen~f_2)~\arrowcomp\gen~f_3 &\ \equiv \ f_1~\arrowcomp\gen~(f_2~\arrowcomp\gen~f_3)
\label{eqn:comp-is-associative}
\\
	(\arrowarr\gen~f_1~\arrowpair\gen~f_2)~\arrowcomp\gen~\arrowarr\gen~snd &\ \equiv \ f_2
\label{eqn:pair-extraction}
\end{align}
\end{displaybreaks}
and distribution of pure computations over effectful:
\begin{displaybreaks}
\begin{align}
	\!\!\!\arrowarr\gen~f_1~\arrowcomp\gen~(f_2~\arrowpair\gen~f_3) &\ \equiv \ 
		(\arrowarr\gen~f_1~\arrowcomp\gen~f_2)~\arrowpair\gen~(\arrowarr\gen~f_1~\arrowcomp\gen~f_3)
\label{eqn:pure-distributes-over-pair}
\\
	\!\!\!\arrowarr\gen~f_1~\arrowcomp\gen~\arrowif\gen~f_2~f_3~f_4 &\ \equiv \
		\arrowif\gen~\lzfcsplit{
			&(\arrowarr\gen~f_1~\arrowcomp\gen~f_2) \\
			&(\arrowarr\gen~f_1~\arrowcomp\gen~f_3) \\
			&(\arrowarr\gen~f_1~\arrowcomp\gen~f_4)}
\label{eqn:pure-distributes-over-if}
\\
	\arrowarr\gen~f_1~\arrowcomp\gen~\arrowlazy\gen~f_2 &\ \equiv \
		\arrowlazy\gen~\fun{0}{\arrowarr\gen~f_1~\arrowcomp\gen~f_2~0}
\label{eqn:pure-distributes-over-lazy}
\end{align}
\end{displaybreaks}

Equivalence between different arrow representations is usually proved in a strongly normalizing $\lambda$-calculus~\cite{cit:lindley-2008entcs-idiom-arrow-monad,cit:lindley-2010jfp-arrow-calculus}, in which every function is free of effects, including nontermination.
Such a $\lambda$-calculus has no need for $\arrowlazy\gen$, so we could not derive~\eqref{eqn:pure-distributes-over-lazy} from existing arrow laws.
We follow Hughes's reasoning~\cite{cit:hughes-2000scp-arrows} for the original arrow laws: it is a function-like property (i.e. it holds for the function arrow), and it cannot not lose, reorder or duplicate effects.

The pair extraction law~\eqref{eqn:pair-extraction}, which \emph{can} be derived from existing arrow laws, is a more problematic, in nonstrict $\lambda$-calculii as well as \lzfclang.
If $f_1$ does not always terminate, using~\eqref{eqn:pair-extraction} to transform a computation can turn a nonterminating expression into a terminating one, or vice-versa.
We could require $f_1$ in the pair extraction law to always terminate.
Instead, we require every argument to $\arrowarr\gen$ to terminate, which simplifies more proofs.

Rather than prove each arrow law for each arrow, we prove arrows are \emph{epimorphic} to arrows for which the laws are known to hold.
(Isomorphism is sufficient but not necessary.)

\begin{definition}[arrow epimorphism]
\label{def:arrow-epimorphism}
An arrow homomorphism $\arrowlift\genb : (x \arrow\gen y) \tto (x \arrow\genb y)$ that has a right inverse is an \mykeyword{arrow epimorphism} from $a$ to $b$.
\end{definition}

\begin{theorem}[epimorphism implies arrow laws]
\label{thm:arrow-epimorphism}
If $\arrowlift\genb : (x \arrow\gen y) \tto (x \arrow\genb y)$ is an arrow epimorphism and the combinators of $a$ define an arrow, then the combinators of $b$ define an arrow.
\end{theorem}
\begin{proof}
Let $\arrowlift\genb^{-1}$ be $\arrowlift\genb$'s right inverse.
For the pair extraction law~\eqref{eqn:pair-extraction},
\begin{displaybreaks}
\begin{align*}
\numberthis
	&(\arrowarr\genb~f_1~\arrowpair\genb~f_2)~\arrowcomp\genb~\arrowarr\genb~snd
\\*
	&\tab\equiv\ (\arrowlift\genb~(\arrowarr\gen~f_1)~\arrowpair\genb~(\arrowlift\genb~(\arrowlift\genb^{-1}~f_2)))~\arrowcomp\genb~\arrowlift\genb~(\arrowarr\gen~snd)
	&&\text{Rewrite with $\arrowlift\genb$}
\\
	&\tab\equiv\ \arrowlift\genb~(\arrowarr\gen~f_1~\arrowpair\gen~\arrowlift\genb^{-1}~f_2)~\arrowcomp\genb~\arrowlift\genb~(\arrowarr\gen~snd)
	&&\text{Homomorphism \eqref{eqn:lift-distributes-over-pair}}
\\
	&\tab\equiv\ \arrowlift\genb~((\arrowarr\gen~f_1~\arrowpair\gen~\arrowlift\genb^{-1}~f_2)~\arrowcomp\gen~\arrowarr\gen~snd)
	&&\text{Homomorphism \eqref{eqn:lift-distributes-over-comp}}
\\
	&\tab\equiv\ \arrowlift\genb~(\arrowlift\genb^{-1}~f_2)
	&&\text{Pair extraction \eqref{eqn:pair-extraction}}
\\*
	&\tab\equiv\ f_2
	&&\text{Right inverse}
\end{align*}
\end{displaybreaks}
The proofs for every other law are similar.
\end{proof}

\subsection{First-Order Let-Calculus Semantics}

\begin{figure*}[!tb]\centering
\smallmathfont
\begin{align*}
	\mathit{p} &\ ::\equiv \ \mathit{x := e};\ ...\ ; \mathit{e}
\\
	\mathit{e} &\ ::\equiv \ \mathit{x~e}\ |\ let~\mathit{e~e}\ |\ env~\mathit{n}\ |\ \mathit{\pair{e,e}}\ |\ fst~\mathit{e}\ |\ snd~\mathit{e}\ |\ if~\mathit{e~e~e}\ |\ \mathit{v}
\\
	\mathit{v} &\ ::\equiv \ \text{[first-order constants]}
\\[-6pt]
\end{align*}
\begin{align*}
\begin{aligned}[t]
	\meaningof{\mathit{x} := \mathit{e};\ ...\ ; \mathit{e_b}}\gen &\ :\equiv\
		\mathit{x} := \meaningof{\mathit{e}}\gen;\ ...\ ; \meaningof{\mathit{e_b}}\gen
\\[6pt]
	\meaningof{\mathit{x}~\mathit{e}}\gen &\ :\equiv\
		\meaningof{\pair{\mathit{e},\pair{}}}\gen~\arrowcomp\gen~\mathit{x}
\\
	\meaningof{\pair{\mathit{e}_1,\mathit{e}_2}}\gen &\ :\equiv\
		\meaningof{\mathit{e}_1}\gen~\arrowpair\gen~\meaningof{\mathit{e}_2}\gen
\\
	\meaningof{fst~\mathit{e}}\gen &\ :\equiv\
		\meaningof{\mathit{e}}\gen~\arrowcomp\gen~\arrowarr\gen~fst
\\
	\meaningof{snd~\mathit{e}}\gen &\ :\equiv\
		\meaningof{\mathit{e}}\gen~\arrowcomp\gen~\arrowarr\gen~snd
\\
	\meaningof{\mathit{v}}\gen &\ :\equiv\ \arrowarr\gen~(const~\mathit{v})
\\[6pt]
	id &\ := \ \fun{a} a
\\
	const~b &\ := \ \fun{a} b
\\
\end{aligned}
&\tab\tab\tab\tab
\begin{aligned}[t]
\\[3pt]
	\meaningof{let~\mathit{e}~\mathit{e_b}}\gen &\ :\equiv\ 
		(\meaningof{\mathit{e}}\gen~\arrowpair\gen~\arrowarr\gen~id)~
			\arrowcomp\gen~
		\meaningof{\mathit{e_b}}\gen
\\
	\meaningof{env~0}\gen &\ :\equiv\ \arrowarr\gen~fst
\\
	\meaningof{env~(\mathit{n}+1)}\gen &\ :\equiv\ \arrowarr\gen~snd~\arrowcomp\gen~\meaningof{env~\mathit{n}}\gen
\\
	\meaningof{if~\mathit{e_c}~\mathit{e_t}~\mathit{e_f}}\gen &\ :\equiv\
		\arrowif\gen~
			\meaningof{\mathit{e_c}}\gen~
			\meaningof{lazy~\mathit{e_t}}\gen~
			\meaningof{lazy~\mathit{e_f}}\gen
\\
	\meaningof{lazy~\mathit{e}}\gen &\ :\equiv\ \arrowlazy\gen~\fun{0}{\meaningof{\mathit{e}}\gen}
\\
\\
	\text{subject to} &\ \meaningof{\mathit{p}}\gen : \pair{} \arrow\gen y \ \text{for some $y$}
\end{aligned}
\end{align*}
\bottomhrule
\caption[First-order semantics]{Interpretation of a let-calculus with first-order definitions and De-Bruijn-indexed bindings as arrow $\mathrm{a}$ computations.
}
\label{fig:semantic-function}
\end{figure*}

\figref{fig:semantic-function} defines a transformation from a first-order let-calculus to arrow computations for any arrow $a$.
A program is a sequence of definition statements followed by a final expression.
The semantic function $\meaningof{\cdot}\gen$ transforms each defining expression and the final expression into arrow computations.
Functions are named, but local variables and arguments are not.
Instead, variables are referred to by De Bruijn indexes, with $0$ referring to the innermost binding.

Perhaps unsurprisingly, interpretations act like stack machines.
A final expression has type $\pair{} \arrow\gen y$, where $y$ is the type of the program's value, and $\pair{}$ denotes an empty list, or stack.
A $let$ expression pushes a value onto the stack.
First-order functions have type $\pair{x,\pair{}} \arrow\gen y$ where $x$ is the argument type and $y$ is the return type.
Application sends a stack containing just an $x$.

We generally regard programs as if they were their final expressions.
Thus, the following definition applies to both programs and expressions.

\begin{definition}[well-defined expression]
\label{def:well-defined-expression}
An expression $\mathit{e}$ is \keyword{well-defined} under arrow $a$ if $\meaningof{\mathit{e}}\gen : x \arrow\gen y$ for some $x$ and $y$, and $\meaningof{\mathit{e}}\gen$ terminates.
\end{definition}

From here on, we assume all expressions are well-defined.
(The arrow $a$ will be clear from context.)
Well-definedness does not guarantee that \emph{running} an interpretation terminates.
It just simplifies statements about expressions, such as the following theorem, on which most of our semantic correctness results rely.

\begin{theorem}[homomorphisms distribute over expressions]
\label{thm:homomorphism-implies-correct}
Let $\arrowlift\genb : (x \arrow\gen y) \tto (x \arrow\genb y)$ be an arrow homomorphism.
For all $\mathit{e}$, $\meaningof{\mathit{e}}\genb\ \equiv\ \arrowlift\genb~\meaningof{\mathit{e}}\gen$.%
\end{theorem}
\begin{proof}
By structural induction.
Base cases proceed by expansion and using $\arrowarr\genb \equiv \arrowlift\genb \circ \arrowarr\gen$~\eqref{eqn:lift-distributes-over-arr}.
For example, for constants:
\begin{align*}
\numberthis
	\meaningof{\mathit{v}}\genb
		&\ \equiv\ \arrowarr\genb~(const~\mathit{v})
		&&\text{Def of $\meaningof{\cdot}\genb$}
\\
		&\ \equiv\ \arrowlift\genb~(\arrowarr\gen~(const~\mathit{v}))
		&&\text{Homomorphism \eqref{eqn:lift-distributes-over-arr}}
\\
		&\ \equiv\ \arrowlift\genb~\meaningof{\mathit{v}}\gen
		&&\text{Def of $\meaningof{\cdot}\gen$}
\end{align*}
Inductive cases proceed by expansion, applying the inductive hypothesis on subterms, and applying distributive laws~\eqref{eqn:lift-distributes-over-comp}--\eqref{eqn:lift-distributes-over-lazy}.
For example, for pairing:
\begin{displaybreaks}
\begin{align*}
\numberthis
	\meaningof{\pair{\mathit{e}_1,\mathit{e}_2}}\genb
		&\ \equiv\ \meaningof{\mathit{e}_1}\genb~\arrowpair\genb~\meaningof{\mathit{e}_2}\genb
		&&\text{Def of $\meaningof{\cdot}\genb$}
\\*
		&\ \equiv\ (\arrowlift\genb~\meaningof{\mathit{e}_1}\gen)~\arrowpair\genb~(\arrowlift\genb~\meaningof{\mathit{e}_2}\gen)
		&&\text{Ind hypothesis}
\\
		&\ \equiv\ \arrowlift\genb~(\meaningof{\mathit{e}_1}\gen~\arrowpair\gen~\meaningof{\mathit{e}_2}\gen)
		&&\text{Homomorphism \eqref{eqn:lift-distributes-over-pair}}
\\
		&\ \equiv\ \arrowlift\genb~\meaningof{\pair{\mathit{e}_1,\mathit{e}_2}}\gen
		&&\text{Def of $\meaningof{\cdot}\gen$}
\end{align*}
\end{displaybreaks}
It is not hard to check the remaining cases.
\end{proof}

If we assume $\arrowlift\genb$ defines correct behavior for arrow $b$ in terms of arrow $a$, and prove that $\arrowlift\genb$ is a homomorphism, then by Theorem~\ref{thm:homomorphism-implies-correct}, $\meaningof{\cdot}\genb$ is correct.
%In other words, \emph{homomorphism implies correctness}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Bottom Arrow}

\begin{figure*}[!tb]\centering
\smallmathfont
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		&X \botto Y \ ::= \ X \tto Y_\bot
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\arrbot : (X \tto Y) \tto (X \botto Y) \\
		&\arrbot~f \ := \ f
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\compbot) : (X \botto Y) \tto (Y \botto Z) \tto (X \botto Z) \\
		&(f_1~\compbot~f_2)~a \ := \ if~(f_1~a = \bot)~\bot~(f_2~(f_1~a))
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\pairbot) : (X \botto {Y_1}) \tto (X \botto {Y_2}) \tto (X \botto \pair{Y_1,Y_2}) \\
		&(f_1~\pairbot~f_2)~a \ := \ 
		\lzfclet{
			b_1 & f_1~a \\
			b_2 & f_2~a
		}{if~(b_1 = \bot~or~b_2 = \bot)~\bot~{\pair{b_1,b_2}}}
	\end{aligned}
\end{aligned}
&\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\ifbot : (X \botto Bool) \tto (X \botto Y) \tto (X \botto Y) \tto (X \botto Y) \\
		&\ifbot~f_1~f_2~f_3~a \ := \
			\lzfccase{f_1~a}{true & f_2~a \\ false & f_3~a \\ \bot & \bot}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\lazybot : (1 \tto (X \botto Y)) \tto (X \botto Y) \\
		&\lazybot~f~a \ := \ f~0~a
	\end{aligned}
\end{aligned}
\end{align*}
\bottomhrule
\caption[Bottom arrow definitions]{Bottom arrow definitions.}
\label{fig:bottom-arrow-defs}
\end{figure*}

Using the diagram in~\eqref{eqn:roadmap-diagram1} as a sort of map, we start in the upper-left corner:
\youarehere{eqn:roadmap-diagram2}
Through Section~\ref{sec:preimage-arrow}, we move across the top to $X \preto Y$.

To use Theorem~\ref{thm:homomorphism-implies-correct} to prove correct the interpretations of expressions as preimage arrow computations, we need the preimage arrow to be homomorphic to a simpler arrow with easily understood behavior.
The function arrow~\eqref{eqn:function-arrow} is an obvious candidate.
However, we will need to explicitly handle nontermination as an error value, so we need a slightly more complicated arrow.

\figref{fig:bottom-arrow-defs} defines the \mykeyword{bottom arrow}.
Its computations have type $X \botto Y ::= X \tto Y_\bot$, where $Y_\bot ::= Y \u \set{\bot}$ and $\bot$ is a distinguished error value.
The type $Bool_\bot$, for example, denotes the members of $Bool \u \set{\bot} = \set{true,false,\bot}$.

To prove the arrow laws, we need a coarser notion of equivalence.

\begin{definition}[bottom arrow equivalence]
Two computations $f_1 : X \botto Y$ and $f_2 : X \botto Y$ are equivalent, or $f_1 \equiv f_2$, when $f_1~a \equiv f_2~a$ for all $a \in X$.
\end{definition}

\begin{theorem}
$\arrbot$, $(\pairbot)$, $(\compbot)$, $\ifbot$ and $\lazybot$ define an arrow.
\end{theorem}
\begin{proof}
The bottom arrow is epimorphic to (in fact, isomorphic to) the maybe monad's Kleisli arrow.
\end{proof}

\section{Deriving the Mapping Arrow}

Computing preimages requires an observable domain, which lambdas do not have.
Further, theorems about functions in set theory tend to be about mappings, not about lambdas that may raise errors.
As in intermediate step, then, we need an arrow whose computations produce mappings or are mappings themselves.

It is tempting to try to make the mapping arrow's computations mapping-valued; i.e. $X \mapto Y ::= X \pto Y$.
Unfortunately, we could not define $\arrmap : (X \tto Y) \tto (X \pto Y)$: to define a mapping, we need a domain, but lambdas' domains are unobservable.

To parameterize mapping arrow computations on a domain, we define the \mykeyword{mapping arrow} computation type as
\begin{equation}
	X \mapto Y \ ::= \ Set~X \tto (X \pto Y)
\end{equation}
The absence of $\bot$ in $Set~X \tto (X \pto Y)$, and the fact that type parameters $X$ and $Y$ denote sets, will make it easier to apply well-known theorems from measure theory, which know nothing of lambda types and propagating error values.

To use Theorem~\ref{thm:homomorphism-implies-correct} to prove that expressions interpreted using $\meaningof{\cdot}\map$ behave correctly with respect to $\meaningof{\cdot}_\bot$, we need to define correctness using a lift from the bottom arrow to the mapping arrow.
It is helpful to have a standalone function $domain_\bot$ that computes the subset of $A$ on which $f$ does not return $\bot$.
We define that first, and then define $\liftmap$ in terms of it:
\begin{align}
	&\begin{aligned}
		&domain_\bot : (X \botto Y) \tto Set~X \tto Set~X \\
		&domain_\bot~f~A \ := \ \setb{a \in A}{f~a \neq \bot}
	\end{aligned} \\
\nonumber\\[-0.5\baselineskip]
	&\begin{aligned}
		&\liftmap : (X \botto Y) \tto (X \mapto Y) \\
		&\liftmap~f~A \ := \ mapping~f~(domain_\bot~f~A)
	\end{aligned}
\end{align}
So $\liftmap~f~A$ is like $mapping~f~A$, except the domain does not contain inputs that produce errors---a good notion of correctness.

\begin{figure*}[!tb]\centering
\smallmathfont
\begin{align*}
\!\!
\begin{aligned}[t]
	&\begin{aligned}[t]
		&range : (X \pto Y) \tto Set~Y \\
		&range~g \ := \ image~snd~g
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\circ\map) : (Y \pto Z) \tto (X \pto Y) \tto (X \pto Z) \\
		&g_2 \circ\map g_1 \ := \ 
			\lzfclet{
				A & preimage~g_1~(domain~g_2)
			}{\fun{a \in A}{g_2~(g_1~a)}}
	\end{aligned}
\end{aligned}
&\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\pair{\cdot,\cdot}\map : (X \pto Y_1) \tto (X \pto Y_2) \tto (X \pto Y_1 \times Y_2) \\
		&\pair{g_1,g_2}\map \ := \ 
			\lzfclet{
				A & domain~g_1 \i domain~g_2
			}{\fun{a \in A}{\pair{g_1~a,g_2~a}}}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\uplus\map) : (X \pto Y) \tto (X \pto Y) \tto (X \pto Y) \\
		&\lzfcsplit{&g_1 \uplus\map g_2 \ := \
		\lzfclet{
				A & domain~g_1 \uplus domain~g_2
			}{\fun{a \in A}{if~(a \in domain~g_1)~(g_1~a)~(g_2~a)}}}
	\end{aligned}
\end{aligned}
\end{align*}
\bottomhrule
\caption[Additional mapping operations]{Additional operations on partial mappings.}
\label{fig:more-mapping-defs}
\end{figure*}

If $\liftmap$ is to be a homomorphism, mapping arrow computation equivalence needs to be more extensional.

\begin{definition}[mapping arrow equivalence]
Two computations $g_1 : X \mapto Y$ and $g_2 : X \mapto Y$ are equivalent, or $g_1 \equiv g_2$, when $g_1~A \equiv g_2~A$ for all $A \subseteq X$.
\end{definition}

\begin{figure*}[!tb]\centering
\smallmathfont
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		X \mapto Y \ ::= \ Set~X \tto (X \pto Y)
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\arrmap : (X \tto Y) \tto (X \mapto Y) \\
		&\arrmap \ := \ \liftmap \circ \arrbot
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\compmap) : (X \mapto Y) \tto (Y \mapto Z) \tto (X \mapto Z) \\
		&(g_1~\compmap~g_2)~A \ := \ 
			\lzfclet{
				g_1' & g_1~A \\
				g_2' & g_2~(range~g_1')
			}{g_2' \circ\map g_1'}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\pairmap) : (X \mapto Y_1) \tto (X \mapto Y_2) \tto (X \mapto \pair{Y_1,Y_2}) \\
		&(g_1~\pairmap~g_2)~A \ := \ \pair{g_1~A,g_2~A}\map
	\end{aligned} \\
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\ifmap : (X \mapto Bool) \tto (X \mapto Y) \tto (X \mapto Y) \tto (X \mapto Y) \\
		&\ifmap~g_1~g_2~g_3~A \ := \ 
			\lzfclet{
				g_1' & g_1~A \\
				g_2' & g_2~(preimage~g_1'~\set{true}) \\
				g_3' & g_3~(preimage~g_1'~\set{false})
			}{g_2' \uplus\map g_3'}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\lazymap : (1 \tto (X \mapto Y)) \tto (X \mapto Y) \\
		&\lazymap~g~A \ := \ if~(A = \emptyset)~\emptyset~(g~0~A)
	\end{aligned} \\
\\[-6pt]
\hline
\\[-6pt]
	&\begin{aligned}[t]
		&\liftmap : (X \botto Y) \tto (X \mapto Y) \\
		&\liftmap~f~A := \setb{\pair{a,b} \in mapping~f~A}{b \neq \bot}
	\end{aligned}
\end{aligned}
\end{align*}
\bottomhrule
\caption[Mapping arrow definitions]{Mapping arrow definitions.}
\label{fig:mapping-arrow-defs}
\end{figure*}

Clearly $\arrmap := \liftmap \circ \arrbot$ meets the first homomorphism law~\eqref{eqn:lift-distributes-over-arr}.
The remainder of this section derives $(\pairmap)$, $(\compmap)$, $\ifmap$ and $\lazymap$ from bottom arrow combinators, in a way that ensures $\liftmap$ is an arrow homomorphism.
\figref{fig:more-mapping-defs} defines the additional necessary mapping operations $range$, composition, pairing, and disjoint union, and \figref{fig:mapping-arrow-defs} contains the resulting mapping arrow combinators.

\subsection{Composition}
Starting with the left side of~\eqref{eqn:lift-distributes-over-comp}, we expand definitions, simplify $f$ by restricting it to a set for which $f_1~a \neq \bot$:
\begin{displaybreaks}
\begin{align*}
\numberthis
	&\liftmap~(f_1~\compbot~f_2)~A
\\*
	&\tab\equiv \ 
		\lzfclet{
			f & \fun{a}{if~(f_1~a = \bot)~\bot~(f_2~(f_1~a))} \\
			A' & domain_\bot~f~A
		}{mapping~f~A'}
	&&\text{Def of $\liftmap$, $(\compbot)$}
\\
	&\tab\equiv \ 
		\lzfclet{
			f & \fun{a}{f_2~(f_1~a)} \\
			A' & domain_\bot~f~(domain_\bot~f_1~A)
		}{mapping~f~A'}
	&&\text{Simplify $f$}
\\
	&\tab\equiv \ 
		\lzfclet{
			A' & \setb{a \in domain_\bot~f_1~A}{f_2~(f_1~a) \neq \bot}
		}{\fun{a \in A'}{f_2~(f_1~a)}}
	&&\text{Def of $domain_\bot$, $mapping$}
\\
\intertext{We finish by converting bottom arrow computations to the mapping arrow and rewriting in terms of mapping composition $(\circ\map)$:}
	&\tab\equiv \ 
		\lzfclet{
			g_1 & \liftmap~f_1~A \\
			A' & preimage~g_1~(domain_\bot~f_2~(range~g_1))
		}{\fun{a \in A'}{f_2~(g_1~a)}}
	&&\text{Rewrite with $\liftmap$}
\\
	&\tab\equiv \ 
		\lzfclet{
			g_1 & \liftmap~f_1~A \\
			g_2 & \liftmap~f_2~(range~g_1) \\
			A' & preimage~g_1~(domain~g_2)
		}{\fun{a \in A'}{g_2~(g_1~a)}}
	&&\text{Rewrite with $\liftmap$}
\\
	&\tab\equiv \ 
		\lzfclet{
			g_1 & \liftmap~f_1~A \\
			g_2 & \liftmap~f_2~(range~g_1)
		}{g_2 \circ\map g_1}
	&&\text{Rewrite with $(\circ\map)$}
\end{align*}
\end{displaybreaks}
Substituting $g_1$ for $\liftmap~f_1$ and $g_2$ for $\liftmap~f_2$ gives a definition for $(\compmap)$ (\figref{fig:mapping-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-comp} holds.

\subsection{Pairing}
Starting with the left side of~\eqref{eqn:lift-distributes-over-pair}, we expand definitions, and simplify $f$ by restricting it to a set for which $f_1~a \neq \bot$ and $f_2~a \neq \bot$:
\begin{align*}
\numberthis
	&\liftmap~(f_1~\pairbot~f_2)~A
\\*
	&\tab\equiv \ 
		\lzfclet{
			f & \fun{a} \lzfclet{
				b_1 & f_1~a \\
				b_2 & f_2~a
			}{if~(b_1 = \bot~or~b_2 = \bot)~\bot~{\pair{b_1,b_2}}}
		\\
			A' & domain_\bot~f~A
		}{mapping~f~A'}
	&&\text{Def of $\liftmap$, $(\pairbot)$}
\\
	&\tab\equiv \ 
		\lzfclet{
			f & \fun{a} \pair{f_1~a,f_2~a} \\
			A' & domain_\bot~f_1~A \i domain_\bot~f_2~A
		}{mapping~f~A'}
	&&\text{Simplify $f$}
\\
	&\tab\equiv \ 
		\lzfclet{
			A' & domain_\bot~f_1~A \i domain_\bot~f_2~A
		}{\fun{a \in A'}\pair{f_1~a,f_2~a}}
	&&\text{Def of $mapping$}
\\
\intertext{We finish by converting bottom arrow computations to the mapping arrow and rewriting in terms of $\pair{\cdot,\cdot}\map$:}
	&\tab\equiv \ 
		\lzfclet{
			g_1 & \liftmap~f_1~A \\
			g_2 & \liftmap~f_2~A \\
			A' & domain~g_1 \i domain~g_2
		}{\fun{a \in A'}{\pair{g_1~a,g_2~a}}}
	&&\text{Rewrite with $\liftmap$}
\\
	&\tab\equiv \ \pair{\liftmap~f_1~A, \liftmap~f_2~A}\map
	&&\text{Rewrite with $\pair{\cdot,\cdot}\map$}
\end{align*}
Substituting $g_1$ for $\liftmap~f_1$ and $g_2$ for $\liftmap~f_2$ gives a definition for $(\pairmap)$ (\figref{fig:mapping-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-pair} holds.

\subsection{Conditional}
Starting with the left side of~\eqref{eqn:lift-distributes-over-if}, we expand definitions, and simplify $f$ by restricting it to a domain for which $f_1~a \neq \bot$:
\begin{displaybreaks}
\begin{align*}
\numberthis
	&\liftmap~(\ifbot~f_1~f_2~f_3)~A
\\*
	&\tab\equiv\ 
		\lzfclet{
			f & \fun{a}{\lzfccase{f_1~a}{true & f_2~a \\ false & f_3~a \\ \bot & \bot}} \\
			A' & domain_\bot~f~A
		}{mapping~f~A'}
	&&\text{Def of $\liftmap$, $\ifbot$}
\\
	&\tab\equiv\ 
		\lzfclet{
			f & \fun{a}{if~(f_1~a)~(f_2~a)~(f_3~a)} \\
			g_1 & mapping~f_1~(domain_\bot~f_1~A) \\
			A_2 & preimage~g_1~\set{true} \\
			A_3 & preimage~g_1~\set{false} \\
			A' & domain_\bot~f_2~A_2 \uplus domain_\bot~f_3~A_3
		}{mapping~f~A'}
	&&\text{Simplify $f$}
\\
	&\tab\equiv\ 
		\lzfclet{
			g_1 & mapping~f_1~(domain_\bot~f_1~A) \\
			A_2 & preimage~g_1~\set{true} \\
			A_3 & preimage~g_1~\set{false} \\
			A' & domain_\bot~f_2~A_2 \uplus domain_\bot~f_3~A_3
		}{\fun{a \in A'} if~(f_1~a)~(f_2~a)~(f_3~a)}
	&&\text{Def of $mapping$}
\\
\intertext{We finish by converting bottom arrow computations to the mapping arrow and rewriting in terms of $(\uplus\map)$:}
	&\tab\equiv \ 
	\lzfclet{
		g_1 & \liftmap~f_1~A \\
		g_2 & \liftmap~f_2~(preimage~g_1~\set{true}) \\
		g_3 & \liftmap~f_3~(preimage~g_1~\set{false}) \\
		A' & domain~g_2 \uplus domain~g_3
	}{\fun{a \in A'}{if~(a \in domain~g_2)~(g_2~a)~(g_3~a)}}
	&&\text{Rewrite with $\liftmap$}
\\
	&\ \equiv \
	\lzfclet{
		g_1 & \liftmap~f_1~A \\
		g_2 & \liftmap~f_2~(preimage~g_1~\set{true}) \\
		g_3 & \liftmap~f_3~(preimage~g_1~\set{false})
	}{g_2 \uplus\map g_3}
	&&\text{Rewrite with $(\uplus\map)$}
\end{align*}
\end{displaybreaks}
Substituting $g_1$ for $\liftmap~f_1$, $g_2$ for $\liftmap~f_2$, and $g_3$ for $\liftmap~f_3$ gives a definition for $\ifmap$ (\figref{fig:mapping-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-if} holds.

\subsection{Laziness}
Starting with the left side of~\eqref{eqn:lift-distributes-over-lazy}, we expand definitions:
\begin{equation}
	\liftmap~(\lazybot~f)~A
	\ \equiv \
		\lzfclet{
			A' & domain_\bot~(\fun{a}{f~0~a})~A
		}{mapping~(\fun{a}{f~0~a})~A'}
\end{equation}
It appears we need an $\eta$ rule to continue, which \lzfclang does not have (i.e. $\fun{\mathit{x}}{\mathit{e}~\mathit{x}} \not\equiv \mathit{e}$ because $\mathit{e}$ may not terminate).
Fortunately, we can use weaker facts.
If $A \neq \emptyset$, then $domain_\bot~(\fun{a}{f~0~a})~A\ \equiv\ domain_\bot~(f~0)~A$.
Further, it terminates if and only if $mapping~(f~0)~A'$ terminates.
Therefore, if $A \neq \emptyset$, we can replace $\fun{a}{f~0~a}$ with $f~0$.
If $A = \emptyset$, then $\liftmap~(\lazybot~f)~A = \emptyset$ (the empty mapping), so
\begin{align*}
\numberthis
	\liftmap~(\lazybot~f)~A
	&\ \equiv \
		if~(A = \emptyset)~\emptyset~(mapping~(f~0)~(domain_\bot~(f~0)~A))
\\
	&\ \equiv \
		if~(A = \emptyset)~\emptyset~(\liftmap~(f~0)~A)
\end{align*}
Substituting $g~0$ for $\liftmap~(f~0)$ gives a $\lazymap$ (\figref{fig:mapping-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-lazy} holds.

\subsection{Correctness}

\begin{theorem}[mapping arrow correctness]
\label{thm:mapping-arrow-correctness}
$\liftmap$ is a homomorphism.%
\end{theorem}
\begin{proof}
By construction.
\end{proof}

\begin{corollary}[semantic correctness]
For all $\mathit{e}$, $\meaningof{\mathit{e}}\map \equiv \liftmap~\meaningof{\mathit{e}}_\bot$.
\end{corollary}

Without restrictions, mapping arrow computations can be quite unruly.
For example, the following computation is well-typed, but returns the identity mapping on $Bool$ when applied to an empty domain, and the empty mapping when applied to any other domain:
\begin{equation}
\begin{aligned}
	&nonmonotone : Bool \mapto Bool \\
	&nonmonotone~A \ := \ if~(A = \emptyset)~(\fun{a \in Bool} a)~\emptyset
\end{aligned}
\end{equation}
It would be nice if we could be sure that every $X \mapto Y$ is not only monotone, but acts as if it returned restricted mappings.
The following equivalent property is easier to state, and makes proving the arrow laws simple.

\begin{definition}[mapping arrow law]
\label{def:mapping-arrow-law}
Let $g : X \mapto Y$. If there exists an $f : X \botto Y$ such that $g \equiv \liftmap~f$, then $g$ obeys the \mykeyword{mapping arrow law}.%
\end{definition}

By homomorphism of $\liftmap$, mapping arrow combinators preserve this law.
It is therefore safe to assume that the mapping arrow law holds for all $g : X \mapto Y$.

\begin{theorem}
$\liftmap$ is an arrow epimorphism.
\end{theorem}
\begin{proof}
Follows from Theorem~\ref{thm:mapping-arrow-correctness} and restriction of $X \mapto Y$ to instances for which the mapping arrow law (Definition~\ref{def:mapping-arrow-law}) holds.
\end{proof}

\begin{corollary}
$\arrmap$, $(\pairmap)$, $(\compmap)$, $\ifmap$ and $\lazymap$ define an arrow.
\end{corollary}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Lazy Preimage Mappings}
\label{sec:lazy-preimage-mappings}

\begin{figure*}[!tb]\centering
\smallmathfont
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		&X \prepto Y ::= \pair{Set~Y, Set~Y \tto Set~X}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&pre : (X \mapto Y) \tto (X \prepto Y) \\
		&pre~g \ := \ \pair{range~g, \fun{B}{preimage~g~B}}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&ap\pre : (X \prepto Y) \tto Set~Y \tto Set~X \\
		&ap\pre~\pair{Y',p}~B \ := \ p~(B \i Y') 
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&domain\pre : (X \prepto Y) \tto Set~X \\
		&domain\pre~\pair{Y',p} \ := \ p~Y'
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&range\pre : (X \prepto Y) \tto Set~Y \\
		&range\pre~\pair{Y',p} \ := \ Y'
	\end{aligned} \\
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\pair{\cdot,\cdot}\pre : (X \prepto Y_1) \tto (X \prepto Y_2) \tto (X \prepto Y_1 \times Y_2) \\
		&\pair{\pair{Y_1',p_1},\pair{Y_2',p_2}}\pre \ := \
		\lzfclet{
			Y' & Y_1' \times Y_2' \\
			p & \fun{B}{\U\limits_{\pair{b_1,b_2} \in B} p_1~\set{b_1} \i p_2~\set{b_2}} \\
		}{\pair{Y',p}}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\circ\pre) : (Y \prepto Z) \tto (X \prepto Y) \tto (X \prepto Z) \\
		&\pair{Z',p_2} \circ\pre h_1 \ := \ \pair{Z', \fun{C}{ap\pre~h_1~(p_2~C)}}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\uplus\pre) : (X \prepto Y) \tto (X \prepto Y) \tto (X \prepto Y) \\
		&h_1 \uplus\pre h_2 \ := \ 
			\lzfclet{
					Y' & range\pre~h_1 \u range\pre~h_2 \\
					p & \fun{B}{ap\pre~h_1~B \uplus ap\pre~h_2~B}
				}{\pair{Y',p}}
	\end{aligned}
\end{aligned}
\end{align*}
\bottomhrule
\caption[Lazy preimage mappings]{Lazy preimage mappings and operations.}
\label{fig:preimage-mapping-defs}
\end{figure*}

On a computer, we do not often have the luxury of testing each function input to see whether it belongs to a preimage set.
Even for finite domains, doing so is often intractable.

If we wish to compute with infinite sets in the language implementation, we will need an abstraction that makes it easy to replace computation on points with computation on sets whose representations allow efficient operations.
Therefore, in the preimage arrow, we confine computation on points to instances of
\begin{equation}
	X \prepto Y \ ::= \ \pair{Set~Y, Set~Y \tto Set~X}
\end{equation}
with the intention to replace $X \prepto Y$ instances with an approximation further on.
Like a mapping, an $X \prepto Y$ has an observable domain---but computing the input-output pairs is delayed.
We therefore call these \mykeyword{lazy preimage mappings}.

Converting a mapping to a lazy preimage mapping requires constructing a delayed application of $preimage$:
\begin{equation}
\begin{aligned}
	&pre : (X \pto Y) \tto (X \prepto Y) \\
	&pre~g \ := \ \pair{range~g,\fun{B}{preimage~g~B}}
\end{aligned}
\end{equation}
To apply a preimage mapping to some $B$, we intersect $B$ with its range and apply the preimage function:
\begin{equation}
\begin{aligned}
	&ap\pre : (X \prepto Y) \tto Set~Y \tto Set~X \\
	&ap\pre~\pair{Y',p}~B \ := \ p~(B \i Y')
\end{aligned}
\end{equation}

Preimage arrow correctness depends on this fact: that using $ap\pre$ to compute preimages is the same as computing them from a mapping using $preimage$.

\begin{lemma}
\label{lem:preimage-restricted-range}
Let $g : X \pto Y$.
For all $B \subseteq Y$ and $Y'$ such that $range~g \subseteq Y' \subseteq Y$, $preimage~g~(B \i Y') = preimage~g~B$.%
\end{lemma}

\begin{theorem}[$ap\pre$ computes preimages]
\label{thm:pre-like-preimage}
Let $g : X \pto Y$. For all $B \subseteq Y$, $ap\pre~(pre~g)~B = preimage~g~B$.%
\end{theorem}
\begin{proof}
Expand definitions and apply Lemma~\ref{lem:preimage-restricted-range} with $Y' = range~g$.
\end{proof}

\figref{fig:preimage-mapping-defs} defines more operations on preimage mappings, including pairing, composition, and disjoint union operations corresponding to the mapping operations in \figref{fig:more-mapping-defs}.
To prove them correct, we need preimage mappings to be equivalent when they compute the same preimages.

\begin{definition}[preimage mapping equivalence]
$h_1 : X \prepto Y$ and $h_2 : X \prepto Y$ are equivalent, or $h_1 \equiv h_2$, when $ap\pre~h_1~B \equiv ap\pre~h_2~B$ for all $B \subseteq Y$.
\end{definition}

Similarly to proving arrows correct, we prove the operations in \figref{fig:preimage-mapping-defs} are correct by proving that $pre$ is a homomorphism (though not an arrow homomorphism): it distributes over mapping operations to yield preimage mapping operations.
The remainder of this section states these distributive properties as theorems and proves them.
We will use these theorems to derive the preimage arrow from the mapping arrow.

\subsection{Composition}

To prove $pre$ distributes over mapping composition, we can make more or less direct use of the fact that $preimage$ distributes over mapping composition.

\begin{lemma}[$preimage$ distributes over $(\circ\map)$]
\label{lem:preimage-under-composition}
Let $g_1 : X \pto Y$ and $g_2 : Y \pto Z$.
For all $C \subseteq Z$, $preimage~(g_2 \circ\map g_1)~C = preimage~g_1~(preimage~g_2~C)$.%
\end{lemma}

\begin{theorem}[$pre$ distributes over $(\circ\map)$]
\label{thm:preimage-mapping-composition}
Let $g_1 : X \pto Y$ and $g_2 : Y \pto Z$.
Then $pre~(g_2 \circ\map g_1) \equiv (pre~g_2) \circ\pre (pre~g_1)$.%
\end{theorem}
\begin{proof}
Let $\pair{Z',p_2} := pre~g_2$ and $C \subseteq Z$.
Starting from the right-hand side of the equivalence,
\begin{displaybreaks}
\begin{align*}
\numberthis
	&ap\pre~((pre~g_2) \circ\pre (pre~g_1))~C
\\*
	&\tab\equiv\ 
		\lzfclet{
			p & \fun{C}{ap\pre~(pre~g_1)~(p_2~C)} \\
			}{p~(C \i Z')}
	&&\text{Def of $ap\pre$, $(\circ\pre)$}
\\
	&\tab\equiv\ ap\pre~(pre~g_1)~(p_2~(C \i Z'))
	&&\text{Def of $p$}
\\
	&\tab\equiv\ ap\pre~(pre~g_1)~(ap\pre~(pre~g_2)~C)
	&&\text{Rewrite with $ap\pre$}
\\
	&\tab\equiv\ preimage~g_1~(preimage~g_2~C)
	&&\text{Theorem~\ref{thm:pre-like-preimage}}
\\
	&\tab\equiv\ preimage~(g_2 \circ\map g_1)~C
	&&\text{Lemma~\ref{lem:preimage-under-composition}}
\\*
	&\tab\equiv\ ap\pre~(pre~(g_2 \circ\map g_1))~C
	&&\text{Theorem~\ref{thm:pre-like-preimage}}
\\[-2.4\baselineskip]
\end{align*}
\end{displaybreaks}
\qedhere
\end{proof}

\subsection{Pairing}

We have less luck with pairing than with composition, because $preimage$ does not distribute over pairing.
Fortunately, $preimage$ distributes over unions, and over pairing and cartesian product together.

\begin{lemma}[$preimage$ distributes over $\pair{\cdot,\cdot}\map$ and $(\times)$]
\label{lem:preimage-under-pairing}
Let $g_1 : X \pto Y_1$ and $g_2 : X \pto Y_2$.
For all $B_1 \subseteq Y_1$ and $B_2 \subseteq Y_2$, $preimage~\pair{g_1,g_2}\map~(B_1 \times B_2) = (preimage~g_1~B_1) \i (preimage~g_2~B_2)$.%
\end{lemma}

\begin{lemma}[$preimage$ distributes over union]
\label{lem:preimage-distributes-over-union}
Let $g : X \pto Y$ and $B : J \tto Set~Y$ be an indexed collection of subsets of $Y$.
Then
\begin{equation}
	\bigcup_{j \in J} preimage~g~(B~j)\ =\ preimage~g~\bigcup_{j \in J} B~j
\end{equation}
\end{lemma}

\begin{theorem}[$pre$ distributes over $\pair{\cdot,\cdot}\map$]
\label{thm:preimage-mapping-pairing}
Let $g_1 : X \pto Y_1$ and $g_2 : X \pto Y_2$. Then $pre~\pair{g_1,g_2}\map \equiv \pair{pre~g_1,pre~g_2}\pre$.%
\end{theorem}
\begin{proof}
Let $\pair{Y_1',p_1} := pre~g_1$, $\pair{Y_2',p_2} := pre~g_2$ and $B \subseteq Y_1 \times Y_2$.
Starting from the right-hand side of the equivalence,
%expand definitions, apply Theorem~\ref{thm:pre-like-preimage}, apply Lemma~\ref{lem:preimage-under-pairing}, note that a product of singletons is a singleton pair, distribute $preimage$ over the union, apply Lemma~\ref{lem:preimage-restricted-range}, and apply Theorem~\ref{thm:pre-like-preimage} again:
\begin{displaybreaks}
\begin{align*}
\numberthis
	&ap\pre~\pair{pre~g_1,pre~g_2}\pre~B 
\\*
	&\tab\equiv \ 
		\lzfclet{
			p & \fun{B}{\U\limits_{\pair{y_1,y_2} \in B} p_1~\set{y_1} \i p_2~\set{y_2}} \\
		}{p~(B \i (Y_1' \times Y_2'))}
	&&\text{Def of $ap\pre$, $\pair{\cdot,\cdot}\pre$}
\\
	&\tab\equiv \U\limits_{\pair{y_1,y_2} \in B \i (Y_1' \times Y_2')} p_1~\set{y_1} \i p_2~\set{y_2}
	&&\text{Def of $p$}
\\
	&\tab\equiv \U\limits_{\pair{y_1,y_2} \in B \i (Y_1' \times Y_2')} preimage~g_1~\set{y_1} \i preimage~g_2~\set{y_2}
	&&\text{Theorem~\ref{thm:pre-like-preimage}}
\\
	&\tab\equiv \U\limits_{\pair{y_1,y_2} \in B \i (Y_1' \times Y_2')} preimage~\pair{g_1,g_2}\map~(\set{y_1} \times \set{y_2})
	&&\text{Lemma~\ref{lem:preimage-under-pairing}}
\\
	&\tab\equiv \U\limits_{\pair{y_1,y_2} \in B \i (Y_1' \times Y_2')} preimage~\pair{g_1,g_2}\map~\set{\pair{y_1,y_2}}
	&&\text{Def of $(\times)$}
\\
	&\tab\equiv \ preimage~\pair{g_1,g_2}\map~(B \i (Y_1' \times Y_2'))
	&&\text{Lemma~\ref{lem:preimage-distributes-over-union}}
\\
	&\tab\equiv \ preimage~\pair{g_1,g_2}\map~B
	&&\text{Lemma~\ref{lem:preimage-restricted-range}}
\\*
	&\tab\equiv \ ap\pre~(pre~\pair{g_1,g_2}\map)~B
	&&\text{Theorem~\ref{thm:pre-like-preimage}}
\end{align*}
\end{displaybreaks}
We have an unmet proof obligation from using Lemma~\ref{lem:preimage-restricted-range}: that $range~\pair{g_1,g_2}\map \subseteq Y_1' \times Y_2'$.

Let $b \in range~\pair{g_1,g_2}\map$.
By definition of $\pair{\cdot,\cdot}\map$, there exists $a \in domain~g_1 \i domain~g_2$ such that $b = \pair{g_1~a,g_2~a}$.
Thus, $b \in Y_1' \times Y_2'$ if and only if $g_1~a \in Y_1'$ and $g_2~a \in Y_2'$.

By definition of $pre$, $Y_1' = range~g_1$ and $Y_2' = range~g_2$.
Because $a \in domain~g_1$, $g_1~a \in range~g_1 = Y_1'$.
Because $a \in domain~g_2$, $g_2~a \in range~g_2 = Y_2'$.
\qedhere
\end{proof}

\subsection{Disjoint Union}

Like proving $pre$ distributes over composition, the proof that it distributes over dijoint union simply lifts a lemma about $preimage$ to lazy preimage mappings.

\begin{lemma}[$preimage$ distributes over $(\uplus\map)$]
\label{lem:preimage-under-piecewise}
Let $g_1 : X \pto Y$ and $g_2 : X \pto Y$ have disjoint domains.
For all $B \subseteq Y$, $preimage~(g_1 \uplus\map g_2)~B = (preimage~g_1~B) \uplus (preimage~g_2~B)$.%
\end{lemma}

\begin{theorem}[$pre$ distributes over $(\uplus\map)$]
\label{thm:piecewise-preimage-mappings}
Let $g_1 : X \pto Y$ and $g_2 : X \pto Y$ have disjoint domains.
Then $pre~(g_1 \uplus\map g_2) \equiv (pre~g_1) \uplus\pre (pre~g_2)$.%
\end{theorem}
\begin{proof}
Let $Y_1' := range~g_1$, $Y_2' := range~g_2$ and $B \subseteq Y$.
Starting from the right-hand side of the equivalence,
\begin{displaybreaks}
\begin{align*}
\numberthis
	&ap\pre~((pre~g_1) \uplus\pre (pre~g_2))~B
\\*
	&\tab\equiv\ 
		\lzfclet{
			p & \fun{B} ap\pre~(pre~g_1)~B \uplus ap\pre~(pre~g_2)~B
		}{p~(B \i (Y_1' \u Y_2'))}
	&&\text{Def of $ap\pre$, $(\uplus\pre)$}
\\
	&\tab\equiv\ ap\pre~(pre~g_1)~(B \i (Y_1' \u Y_2')) \uplus ap\pre~(pre~g_2)~(B \i (Y_1' \u Y_2'))
	&&\text{Def of $p$}
\\
	&\tab\equiv\ preimage~g_1~(B \i (Y_1' \u Y_2')) \uplus preimage~g_2~(B \i (Y_1' \u Y_2'))
	&&\text{Theorem~\ref{thm:pre-like-preimage}}
\\
	&\tab\equiv\ preimage~(g_1 \uplus\map g_2)~(B \i (Y_1' \u Y_2'))
	&&\text{Lemma~\ref{lem:preimage-under-piecewise}}
\\
	&\tab\equiv\ preimage~(g_1 \uplus\map g_2)~B
	&&\text{Lemma~\ref{lem:preimage-restricted-range}}
\\*
	&\tab\equiv\ ap\pre~(pre~(g_1 \uplus\map g_2))~B
	&&\text{Theorem~\ref{thm:pre-like-preimage}}
\end{align*}
\end{displaybreaks}
We have an unmet proof obligation from using Lemma~\ref{lem:preimage-restricted-range}: that $range~(g_1 \uplus\map g_2) \subseteq Y_1' \u Y_2'$.

Let $b \in range~(g_1 \uplus\map g_2)$.
By definition of $(\uplus\map)$, there exists $a \in domain~g_1 \uplus domain~g_2$ such that if $a \in domain~g_1$ then $b = g_1~a$ so $b \in range~g_1 = Y_1'$, and if $a \in domain~g_2$ then $b = g_2~a$ so $b \in range~g_2 = Y_2'$.
Thus $b \in Y_1' \u Y_2'$.
\qedhere
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Deriving the Preimage Arrow}
\label{sec:preimage-arrow}

Now we can define an arrow that runs expressions backwards on sets of outputs.
Its computations should produce preimage mappings or be preimage mappings.

As with the mapping arrow and mappings, we cannot have $X \preto Y ::= X \prepto Y$: we run into trouble trying to define $\arrpre$ because a preimage mapping needs an observable range.
To get one, it is easiest to parameterize preimage computations on a $Set~X$; therefore the \mykeyword{preimage arrow} type constructor is
\begin{equation}
	X \preto Y \ ::= \ Set~X \tto (X \prepto Y)
\end{equation}
or $Set~X \tto \pair{Set~Y, Set~Y \tto Set~X}$.
To deconstruct the type, a preimage arrow computation computes a range first, and returns the range and a lambda that computes preimages.
\figref{fig:arrow-diagram} illustrates this as a circuit diagram.

\begin{figure*}[tb!]\centering
\includegraphics[width=5.5in]{figures/preimage-arrow-diagram}
\caption[Comparison of arrows used as target categories]{Comparison of arrows used as target categories. Computations $f : X \botto Y$ may return an error value $\bot$. Computations $g : X \mapto Y$ produce partial mappings on a given $A \subseteq X$, leaving out inputs for which $f$ would return an error. Computations $h : X \preto Y$ produce lazy preimage mappings; i.e. $h~A$ computes preimages under $g~A$.}
\label{fig:arrow-diagram}
\end{figure*}

To use Theorem~\ref{thm:homomorphism-implies-correct}, we need to define correctness using a lift from the mapping arrow to the preimage arrow.
A simple candidate with the right type is
\begin{equation}
\begin{aligned}
	&\liftpre : (X \mapto Y) \tto (X \preto Y) \\
	&\liftpre~g~A \ := \ pre~(g~A)
\end{aligned}
\end{equation}
By definition of $\liftpre$ and Theorem~\ref{thm:pre-like-preimage}, for all $g : X \mapto Y$, and $A \subseteq X$ and $B \subseteq Y$,
\begin{equation}
\begin{aligned}
	ap\pre~(\liftpre~g~A)~B
		&\ \equiv \ ap\pre~(pre~(g~A))~B
\\
		&\ \equiv \ preimage~(g~A)~B
\end{aligned}
\end{equation}
Thus, lifted mapping arrow computations correctly compute preimages under restricted mappings, exactly as we should expect them to.

To derive the preimage arrow's combinators in a way that makes $\liftpre$ a homomorphism, we need preimage arrow equivalence to mean ``computes the same preimages.''

\begin{definition}[preimage arrow equivalence]
Two computations $h_1 : X \preto Y$ and $h_2 : X \preto Y$ are equivalent, or $h_1 \equiv h_2$, when 
$h_1~A \equiv h_2~A$ for all $A \subseteq X$.
\end{definition}

As with $\arrmap$, defining $\arrpre$ as a composition meets~\eqref{eqn:lift-distributes-over-arr}.
The remainder of this section derives $(\pairpre)$, $(\comppre)$, $\ifpre$ and $\lazypre$ from mapping arrow combinators, in a way that ensures $\liftpre$ is an arrow homomorphism from the mapping arrow to the preimage arrow. \figref{fig:preimage-arrow-defs} contains the resulting definitions.

\begin{figure*}[!tb]\centering
\smallmathfont
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		&X \preto Y ::= Set~X \tto (X \prepto Y)
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\arrpre : (X \tto Y) \tto (X \preto Y) \\
		&\arrpre \ := \ \liftpre \circ \arrmap
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\comppre) : (X \preto Y) \tto (Y \preto Z) \tto (X \preto Z) \\
		&(h_1~\comppre~h_2)~A \ := \ 
			\lzfclet{
				h_1' & h_1~A \\
				h_2' & h_2~(range\pre~h_1')
			}{h_2' \circ\pre h_1'}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\pairpre) : (X \preto Y) \tto (X \preto Z) \tto (X \preto Y \times Z) \\
		&(h_1~\pairpre~h_2)~A \ := \ \pair{h_1~A,h_2~A}\pre
	\end{aligned}
\end{aligned}
&\tab\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\ifpre : (X \preto Bool) \tto (X \preto Y) \tto (X \preto Y) \tto (X \preto Y) \\
		&\ifpre~h_1~h_2~h_3~A \ := \ 
			\lzfclet{
				h_1' & h_1~A \\
				h_2' & h_2~(ap\pre~h_1'~\set{true}) \\
				h_3' & h_3~(ap\pre~h_1'~\set{false})
			}{h_2' \uplus\pre h_3'}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\lazypre : (1 \tto (X \preto Y)) \tto (X \preto Y) \\
		&\lazypre~h~A \ := \ if~(A = \emptyset)~(pre~\emptyset)~(h~0~A)
	\end{aligned} \\
\\[-6pt]
\hline
\\[-6pt]
	&\begin{aligned}[t]
		&\liftpre : (X \mapto Y) \tto (X \preto Y) \\
		&\liftpre~g~A \ := \ pre~(g~A)
	\end{aligned}
\end{aligned}
\end{align*}
\bottomhrule
\caption[Preimage arrow definitions]{Preimage arrow definitions.}
\label{fig:preimage-arrow-defs}
\end{figure*}

\subsection{Composition}

Starting with the left-hand side of~\eqref{eqn:lift-distributes-over-comp},
\begin{displaybreaks}
\begin{align*}
\numberthis
	&ap\pre~(\liftpre~(g_1~\compmap~g_2)~A)~C
\\*
	&\tab\equiv \ 
		\lzfclet{
			g_1' & g_1~A \\
			g_2' & g_2~(range~g_1')
		}{ap\pre~(pre~(g_2' \circ\map g_1'))~C}
	&&\text{Def of $\liftpre$, $(\compmap)$}
\\
	&\tab\equiv \ 
		\lzfclet{
			g_1' & g_1~A \\
			g_2' & g_2~(range~g_1')
		}{ap\pre~((pre~g_1') \circ\pre (pre~g_2'))~C}
	&&\text{Theorem~\ref{thm:preimage-mapping-composition}}
\\
	&\tab\equiv \
		\lzfclet{
			h_1 & \liftpre~g_1~A \\
			h_2 & \liftpre~g_2~(range\pre~h_1)
		}{ap\pre~(h_2 \circ\pre h_1)~C}
	&&\text{Rewrite with $\liftpre$}
\end{align*}
\end{displaybreaks}
Substituting $h_1$ for $\liftpre~g_1$ and $h_2$ for $\liftpre~g_2$, and removing the application of $ap\pre$ from both sides of the equivalence gives a definition of $(\comppre)$ (\figref{fig:preimage-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-comp} holds.

\subsection{Pairing}

Starting with the left-hand side of~\eqref{eqn:lift-distributes-over-pair},
\begin{align*}
\numberthis
	&ap\pre~(\liftpre~(g_1~\pairmap~g_2)~A)~B
\\*
	&\tab\equiv \ ap\pre~(pre~\pair{g_1~A, g_2~A}\map)~B
	&&\text{Def of $\liftpre$, $(\pairmap)$}
\\
	&\tab\equiv \ ap\pre~\pair{pre~(g_1~A), pre~(g_2~A)}\pre~B
	&&\text{Theorem~\ref{thm:preimage-mapping-pairing}}
\\
	&\tab\equiv \ ap\pre~\pair{\liftpre~g_1~A, \liftpre~g_2~A}\pre~B
	&&\text{Rewrite with $\liftpre$}
\end{align*}
Substituting $h_1$ for $\liftpre~g_1$ and $h_2$ for $\liftpre~g_2$, and removing the application of $ap\pre$ from both sides of the equivalence gives a definition of $(\pairpre)$ (\figref{fig:preimage-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-pair} holds.

\subsection{Conditional}

Starting with the left-hand side of~\eqref{eqn:lift-distributes-over-if},
\begin{displaybreaks}
\begin{align*}
\numberthis
	&ap\pre~(\liftpre~(\ifmap~g_1~g_2~g_3)~A)~B
\\*
	&\tab\equiv \ 
		\lzfclet{
			g_1' & g_1~A \\
			g_2' & g_2~(preimage~g_1'~\set{true}) \\
			g_3' & g_3~(preimage~g_1'~\set{false})
		}{ap\pre~(pre~(g_2' \uplus\map g_3'))~B}
	&&\text{Def of $\liftpre$, $\ifmap$}
\\
	&\tab\equiv \ 
		\lzfclet{
			g_1' & g_1~A \\
			g_2' & g_2~(preimage~g_1'~\set{true}) \\
			g_3' & g_3~(preimage~g_1'~\set{false})
		}{ap\pre~((pre~g_2') \uplus\pre (pre~g_3'))~B}
	&&\text{Theorem~\ref{thm:piecewise-preimage-mappings}}
\\
	&\tab\equiv \ 
		\lzfclet{
			g_1' & g_1~A \\
			g_2' & g_2~(ap\pre~(pre~g_1')~\set{true}) \\
			g_3' & g_3~(ap\pre~(pre~g_1')~\set{false})
		}{ap\pre~((pre~g_2') \uplus\pre (pre~g_3'))~B}
	&&\text{Theorem~\ref{thm:pre-like-preimage}}
\\
	&\tab\equiv \ 
		\lzfclet{
			h_1 & \liftpre~g_1~A \\
			h_2 & \liftpre~g_2~(ap\pre~h_1~\set{true}) \\
			h_3 & \liftpre~g_3~(ap\pre~h_1~\set{false})
		}{ap\pre~(h_2 \uplus\pre h_3)~B}
	&&\text{Rewrite with $\liftpre$}
\end{align*}
\end{displaybreaks}
Substituting $h_1$, $h_2$ and $h_3$ for $\liftpre~g_1$, $\liftpre~g_2$ and $\liftpre~g_3$, and removing the application of $ap\pre$ from both sides of the equivalence gives a definition of $\ifpre$ (\figref{fig:preimage-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-if} holds.

\subsection{Laziness}

Starting with the left-hand side of~\eqref{eqn:lift-distributes-over-lazy},
\begin{displaybreaks}
\begin{align*}
\numberthis
	&ap\pre~(\liftpre~(\lazymap~g)~A)~B
\\*
	&\tab\equiv \
		\lzfclet{
			g' & if~(A = \emptyset)~\emptyset~(g~0~A)
		}{ap\pre~(pre~g')~B}
	&&\text{Def of $\liftpre$, $\lazymap$}
\\
	&\tab\equiv \
		\lzfclet{
			h & if~(A = \emptyset)~(pre~\emptyset)~(pre~(g~0~A))
		}{ap\pre~h~B}
	&&\text{Dist $pre$ over $if$}
\\
	&\tab\equiv \
		\lzfclet{
			h & if~(A = \emptyset)~(pre~\emptyset)~(\liftpre~(g~0)~A)
		}{ap\pre~h~B}
	&&\text{Rewrite with $\liftpre$}
\end{align*}
\end{displaybreaks}
Substituting $h~0$ for $\liftpre~(g~0)$ and removing the application of $ap\pre$ from both sides of the equivalence gives a definition for $\lazypre$ (\figref{fig:preimage-arrow-defs}) for which~\eqref{eqn:lift-distributes-over-lazy} holds.

\subsection{Correctness}

\begin{theorem}[preimage arrow correctness]
\label{thm:preimage-arrow-correctness}
$\liftpre$ is a homomorphism.%
\end{theorem}
\begin{proof}
By construction.
\end{proof}

\begin{corollary}[semantic correctness]
\label{cor:preimage-arrow-correctness}
For all $\mathit{e}$, $\meaningof{\mathit{e}}\pre \equiv \liftpre~\meaningof{\mathit{e}}\map$.%
\end{corollary}

As with the mapping arrow, preimage arrow computations can be unruly.
We would like to assume that each $h : X \preto Y$ acts as if it computes preimages under restricted mappings.
The following equivalent property is easier to state, and makes proving the arrow laws simple.

\begin{definition}[preimage arrow law]
\label{def:preimage-arrow-law}
Let $h : X \preto Y$. If there exists a $g : X \mapto Y$ such that $h \equiv \liftpre~g$, then $h$ obeys the \mykeyword{preimage arrow law}.%
\end{definition}

By homomorphism of $\liftpre$, preimage arrow combinators preserve this law.
It is therefore safe to assume that the preimage arrow law holds for all $h : X \preto Y$.

\begin{theorem}
$\liftpre$ is an arrow epimorphism.
\end{theorem}
\begin{proof}
Follows from Theorem~\ref{thm:preimage-arrow-correctness} and restriction of $X \preto Y$ to instances for which the preimage arrow law (Definition~\ref{def:preimage-arrow-law}) holds.
\end{proof}

\begin{corollary}
$\arrpre$, $(\pairpre)$, $(\comppre)$, $\ifpre$ and $\lazypre$ define an arrow.
\end{corollary}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Preimages Under Partial, Probabilistic Functions}

We have defined everything on the top of our roadmap:
\youarehere{eqn:roadmap-diagram3}
and proved that $\liftmap$ and $\liftpre$ are homomorphisms.
At this point, we can interpret an expression $\mathit{e}$ in three ways using the same semantic function for first-order programs:
\begin{enumerate}
	\item As $\meaningof{\mathit{e}}_\bot : X \botto Y$, an intensional function that may raise errors.
	\item As $\meaningof{\mathit{e}}\map : X \mapto Y$, which produces mappings, or extensional functions, on a restricted domain (correct by homomorphism of $\liftmap$).
	\item As $\meaningof{\mathit{e}}\pre : X \preto Y$, which computes preimages under mappings produced by $\meaningof{\mathit{e}}\map$ (correct by homomorphism of $\liftpre$).
\end{enumerate}
These interpretations have two shortcomings:
\begin{enumerate}
	\item They do not pass an implicit random source through $\mathit{e}$'s subexpressions.
	\item Using them requires knowing the set of inputs on which $\mathit{e}$ terminates. If $\meaningof{\mathit{e}}_\bot$ does not terminate on just one input in $A \subseteq X$, neither $\meaningof{\mathit{e}}\map~A$ nor $\meaningof{\mathit{e}}\pre~A$ terminates.
\end{enumerate}
In this section, we define the arrows on the bottom of the roadmap~\eqref{eqn:roadmap-diagram3} by transforming the arrows on the top into arrows that pass an implicit random source and always terminate.
Their correctness again comes down to proving that the combinators between them are homomorphisms, though guaranteed termination needs special treatment.

\subsection{Motivation}

Probabilistic functions that may not terminate, but do so with probability 1, are common.
For example, suppose $random$ retrieves numbers in $[0,1]$ from an implicit random source.
The following probabilistic function defines the well-known geometric distribution by counting the number of times $random < p$:
\begin{equation}
	geometric~p \ := \ if~(random < p)~0~(1 + geometric~p)
\label{eqn:geometric-def}
\end{equation}
For any $p > 0$, $geometric~p$ may not terminate, but the probability of never taking the ``else'' branch is $(1-p) \cdot (1-p) \cdot (1-p) \cdot \cdots = 0$. Thus, $geometric~p$ terminates with probability $1$.

Suppose we interpret $geometric~p$ as $h : \Omega \preto \Nat$, a preimage arrow computation from random sources $\omega \in \Omega$ to naturals, and we have a probability measure $P : Set~\Omega \pto [0,1]$.
The probability of $N \subseteq \Nat$ is then $P~(ap\pre~(h~\Omega)~N)$.
To compute this, we must
\begin{itemize}
	\item Ensure $ap\pre~(h~\Omega)~N$ terminates.
	\item Ensure each $\omega \in \Omega$ contains enough random numbers.
	\item Determine how $random$ indexes numbers in $\omega$.
\end{itemize}
Ensuring $ap\pre~(h~\Omega)~N$ terminates is the most difficult, but doing the other two will provide structure that makes it much easier.

\subsection{Threading and Indexing}
\label{sec:threading-and-indexing}

To ensure random sources contain enough numbers, they should be infinite.

Typically, to thread a random source $\omega \in \Omega$ through computations, $\omega$ is made an infinite stream.
Each computation receives and returns an $\omega$.
The interpretation of $random$ as a computation takes $\omega$'s head and returns its tail.
Combinators pass $\omega$ unchanged to one subcomputation, and pass the resulting $\omega'$ unchanged to the next.
This is typically done with a monad, and it imposes a total order on evaluation.

A little-used alternative that imposes only a partial order makes $\omega$ an infinite binary tree.
Each computation receives an $\omega$ but does not return one.
The interpretation of $random$ as a computation simply returns $\omega$'s root value.
Combinators ignore the root, split $\omega$ into a left subtree $\omega_{left}$ and a right subtree $\omega_{right}$, and pass each to their subcomputations.

Arrows can thread a stream or a tree in the same manner, but the resulting combinators have large definitions, and are conceptually difficult and hard to manipulate.
Fortunately, it is relatively easy to assign each arrow computation a unique index into a tree-shaped random source and pass the random source unchanged.
To do this, we need an indexing scheme.

\begin{definition}[binary indexing scheme]
Let $J$ be an index set, $j_0 \in J$ a distinguished element, and $left : J \tto J$ and $right : J \tto J$ be total, injective functions. If for all $j \in J$, $j = next~j_0$ for some finite composition $next$ of $left$ and $right$, then $J$, $j_0$, $left$ and $right$ define a \mykeyword{binary indexing scheme}.
\end{definition}

For example, let $J$ be the set of lists of $\set{0,1}$, $j_0 := \pair{}$, and $left~j := \pair{0,j}$ and $right~j := \pair{1,j}$.
Alternatively, let $J$ be the set of dyadic rationals in $(0,1)$ (i.e. those with power-of-two denominators), $j_0 := \tfrac{1}{2}$ and
\begin{equation}
\begin{aligned}
	left~(p/q) &\ := \ (p-\tfrac{1}{2})/q
\\
	right~(p/q) &\ := \ (p+\tfrac{1}{2})/q
\end{aligned}
\end{equation}
With this alternative, left-to-right evaluation order can be made to correspond with the natural order $(<)$ over $J$.

In any case, $J$ is countable, and can be thought of as a set of indexes into an infinite binary tree.
Values of type $J \to A$ encode such trees of values in $A$ as total mappings (i.e. infinite vectors).

\subsection{Applicative, Associative Store Transformer}

\begin{figure*}[!tb]\centering
\smallmathfont
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		x \arrow\genc y \ ::= \ AStore~s~(x \arrow\gen y) \ ::= \ J \tto (\pair{s,x} \arrow\gen y)
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\arrowarr\genc : (x \tto y) \tto (x \arrow\genc y) \\
		&\arrowarr\genc \ := \ \arrowtrans\genc \circ \arrowarr\gen
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\arrowcomp\genc) : (x \arrow\genc y) \tto (y \arrow\genc z) \tto (x \arrow\genc z) \\
		&(k_1~\arrowcomp\genc~k_2)~j \ := \\
			&\tab(\arrowarr\gen~fst~\arrowpair\gen~k_1~(left~j))~\arrowcomp\gen~k_2~(right~j)
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\arrowpair\genc) : (x \arrow\genc y_1) \tto (x \arrow\genc y_2) \tto (x \arrow\genc \pair{y_1,y_2}) \\
		&(k_1~\arrowpair\genc~k_2)~j \ := \ k_1~(left~j)~\arrowpair\gen~k_2~(right~j)
	\end{aligned} \\
\end{aligned}
&\tab\ 
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\arrowif\genc : (x \arrow\genc Bool) \tto (x \arrow\genc y) \tto (x \arrow\genc y) \tto (x \arrow\genc y) \\
		&\arrowif\genc~k_1~k_2~k_3~j \ := \ 
			\lzfcsplit{\arrowif\gen~&(k_1~(left~j)) \\ &(k_2~(left~(right~j))) \\ &(k_3~(right~(right~j)))}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\arrowlazy\genc : (1 \tto (x \arrow\genc y)) \tto (x \arrow\genc y) \\
		&\arrowlazy\genc~k~j \ := \ \arrowlazy\gen~\fun{0}{k~0~j}
	\end{aligned} \\
\\[-8pt]
\hline
\\[-8pt]
	&\begin{aligned}[t]
		&\arrowtrans\genc : (x \arrow\gen y) \tto (x \arrow\genc y) \\
		&\arrowtrans\genc~f~j \ := \ \arrowarr\gen~snd~\arrowcomp\gen~f
	\end{aligned}
\end{aligned}
\end{align*}
\bottomhrule
\caption[Associative store arrow transformer]{$AStore$ (associative store) arrow transformer definitions.}
\label{fig:astore-arrow-defs}
\end{figure*}

We thread infinite binary trees through bottom, mapping, and preimage arrow computations by defining an \keyword{arrow transformer}: a type constructor that receives and produces an arrow type, and combinators for arrows of the produced type.
The applicative store arrow transformer's type constructor takes a store type $s$ and an arrow type $x \arrow\gen y$:
\begin{equation}
	AStore~s~(x \arrow\gen y) \ ::= \ J \tto (\pair{s,x} \arrow\gen y)
\end{equation}
Reading the type, we see that computations receive an index $j \in J$ and produce a computation that receives a store as well as an $x$.
Lifting extracts the $x$ from the input pair and sends it on to the original computation, ignoring $j$:
\begin{equation}
\begin{aligned}
	&\arrowtrans\genc : (x \arrow\gen y) \tto AStore~s~(x \arrow\gen y) \\
	&\arrowtrans\genc~f~j \ := \ \arrowarr\gen~snd~\arrowcomp\gen~f
\end{aligned}
\end{equation}

\figref{fig:astore-arrow-defs} defines the remaining combinators.
Each subcomputation receives $left~j$, $right~j$, or some other unique binary index.
We thus think of programs interpreted as $AStore$ arrows as being completely unrolled into an infinite binary tree, with each subcomputation labeled with its tree index.

\subsection{Partial, Probabilistic Programs}
\label{sec:probabilistic-programs}

To interpret probabilistic programs, we put an infinite random tree in the store.

\begin{definition}[random source]
Let $\Omega := J \to [0,1]$.
A \keyword{random source} is any infinite binary tree $\omega \in \Omega$.
\end{definition}

\begin{figure*}[!tb]\centering
\includegraphics[width=5in]{figures/random-tree}
\caption[A random source $\omega \in \Omega$]{An $\omega \in \Omega$ is an infinite binary tree of random values encoded as a total mapping from tree indexes in $J$ to real numbers in $[0,1]$.}
\label{fig:omega-tree}
\end{figure*}

\figref{fig:omega-tree} illustrates a single $\omega \in \Omega$.

To interpret partial programs, we need to ensure termination.
One ultimately implementable way is to have the store dictate which branch of each conditional, if any, is taken.

\begin{definition}[branch trace]
A \mykeyword{branch trace} is any $t : J \to Bool_\bot$ such that $t~j = true$ or $t~j = false$ for no more than finitely many $j \in J$.

Let $T \subset J \to Bool_\bot$ be the largest set of branch traces.
\end{definition}

Now $X \arrow\genc Y\ ::=\ AStore~(\Omega \times T)~(X \arrow\gen Y)$ is an $AStore$ arrow type whose computations thread both random stores and branch traces.

For probabilistic programs, we define a combinator $random\genc$ that returns the number at its tree index in the random source, and extend $\meaningof{\cdot}\genc$ for arrows $a^*$ for which $random\genc$ is defined:
\begin{equation}
\begin{aligned}
	&random\genc : X \arrow\genc [0,1] \\
	&random\genc~j \ := \ \arrowarr\gen~(fst~\arrowcomp~fst~\arrowcomp~\pi~j) \\[6pt]
	&\meaningof{random}\genc \ :\equiv \ random\genc
\end{aligned}
\end{equation}
Here, $\pi~j$ projects its argument onto the argument's $j$th coordinate:
\begin{equation}
\begin{aligned}
	&\pi : J \tto (J \to X) \tto X \\
	&\pi~j~f \ := \ f~j
\end{aligned}
\end{equation}
So $\pi~j$ is analogous to $fst$ and $snd$ for pairs, but for vectors at index $j$.

For partial programs, we define a combinator that reads branch traces, and an if-then-else combinator that ensures its test expression agrees with the trace:
\begin{equation}
\begin{aligned}
	&\begin{aligned}
		&branch\genc : X \arrow\genc Bool \\
		&branch\genc~j \ := \ \arrowarr\gen~(fst~\arrowcomp~snd~\arrowcomp~\pi~j)
	\end{aligned} \\
\\[-0.5\baselineskip]
	&\begin{aligned}
		&\arrowconvif\genc : (x \arrow\genc Bool) \tto (x \arrow\genc y) \tto (x \arrow\genc y) \tto (x \arrow\genc y) \\
		&\arrowconvif\genc~k_1~k_2~k_3~j \ := \
			\arrowif\gen~\lzfcsplit{
				&((k_1~(left~j)~\arrowpair\gen~branch\genc~j)~\arrowcomp\gen~\arrowarr\gen~agrees) \\
				&(k_2~(left~(right~j))) \\
				&(k_3~(right~(right~j)))
			}
	\end{aligned}
\end{aligned}
\label{eqn:ifppre-def}
\end{equation}
where $agrees~\pair{b_1,b_2} := if~(b_1 = b_2)~b_1~\bot$.
Thus, if the branch trace does not agree with the test expression, it returns an error.
We define a new semantic function $\meaningofconv{\cdot}\genc$ by replacing the $if$ rule in $\meaningof{\cdot}\genc$:
\begin{equation}
\begin{aligned}
	\meaningofconv{if~\mathit{e_c}~\mathit{e_t}~\mathit{e_f}}\genc &\ :\equiv\
		\arrowconvif\genc~
			\meaningofconv{\mathit{e_c}}\genc~
			\meaningofconv{lazy~\mathit{e_t}}\genc~
			\meaningofconv{lazy~\mathit{e_f}}\genc
\end{aligned}
\end{equation}

For an $AStore$ computation $k$, we obviously must run $k$ on every branch trace in $T$ and filter out $\bot$, or somehow find inputs $\pair{\pair{\omega,t},a}$ for which $agrees$ never returns $\bot$.
Preimage $AStore$ arrows do the former by first computing an image, and the latter by computing preimages of sets that cannot contain $\bot$.

\begin{definition}[terminating, probabilistic arrows]
Define
\begin{equation}
\begin{aligned}
	X \pbotto Y &\ ::=\ AStore~(\Omega \times T)~(X \botto Y) \\
	X \pmapto Y &\ ::=\ AStore~(\Omega \times T)~(X \mapto Y) \\
	X \ppreto Y &\ ::=\ AStore~(\Omega \times T)~(X \preto Y) \\
\end{aligned}
\end{equation}
as the type constructors for the \mykeyword{bottom*}, \mykeyword{mapping*} and \mykeyword{preimage* arrows}.
\end{definition}

\subsection{Correctness}

We have two arrow lifts to prove homomorphic: one from pure computations to effectful (i.e. from those that do not access the store to those that do), and one from effectful computations to effectful.
For both, we need $AStore$ arrow equivalence to be more extensional.

\begin{definition}[$AStore$ arrow equivalence]
Two $AStore$ arrow computations $k_1$ and $k_2$ are equivalent, or $k_1 \equiv k_2$, when $k_1~j \equiv k_2~j$ for all $j \in J$.
\end{definition}

\subsubsection{Pure Expressions}
Proving $\arrowtrans\genc$ is a homomorphism proves $\meaningof{\cdot}\genc$ correctly interprets pure expressions.
Because $AStore$ accepts any arrow type $x \arrow\gen y$, we can do so using only the arrow laws.
From here on, we assume every $AStore$ arrow's base type's combinators obey the arrow laws listed in Section~\ref{sec:arrow-definitions}.

\begin{theorem}[pure $AStore$ arrow correctness]
$\arrowtrans\genc$ is a homomorphism.
\end{theorem}
\begin{proof}
Defining $\arrowarr\genc$ as a composition clearly meets the first homomorphism law~\eqref{eqn:lift-distributes-over-arr}.
For homomorphism laws~\eqref{eqn:lift-distributes-over-comp}--\eqref{eqn:lift-distributes-over-if}, start from the right side, expand definitions, and use arrow laws~\eqref{eqn:pair-extraction}--\eqref{eqn:pure-distributes-over-if} to factor out $\arrowarr\gen~snd$.

For~\eqref{eqn:lift-distributes-over-lazy}, additionally $\beta$-reduce within the outer thunk, then use the lazy distributive law~\eqref{eqn:pure-distributes-over-lazy} to extract $\arrowarr\gen~snd$.
\end{proof}

\begin{corollary}[pure semantic correctness]
\label{cor:pure-astore-semantic-correctness}
For all pure $\mathit{e}$, $\meaningof{\mathit{e}}\genc \equiv \arrowtrans\genc~\meaningof{\mathit{e}}\gen$.
\end{corollary}

\subsubsection{Effectful Expressions}
To prove all interpretations of effectful expressions correct, we need a lift between $AStore$ arrows.
Let $x \arrow\genc y ::= AStore~s~(x \arrow\gen y)$ and $x \arrow\gend y ::= AStore~s~(x \arrow\gend y)$.
Define
\begin{equation}
\begin{aligned}
	&\arrowlift\gend : (x \arrow\genc y) \tto (x \arrow\gend y) \\
	&\arrowlift\gend~f~j \ := \ \arrowlift\genb~(f~j)
\end{aligned}
\end{equation}
where $\arrowlift\genb : (x \arrow\gen y) \tto (x \arrow\genb y)$.
A commutative diagram shows the relationships more clearly:
\begin{equation}
\begin{CD}
	x \arrow\gen y @>{\arrowlift\genb}>> x \arrow\genb y \\
	@V{\arrowtrans\genc}VV @VV{\arrowtrans\gend}V \\
	x \arrow\genc y @>>{\arrowlift\gend}> x \arrow\gend y
\end{CD}
\label{eqn:lift-diagram}
\end{equation}
At minimum, we should expect to produce equivalent $x \arrow\gend y$ computations from $x \arrow\gen y$ computations whether a $\arrowlift$ or an $\arrowtrans$ is done first.

\begin{theorem}[natural transformation]
If $\arrowlift\genb$ is an arrow homomorphism, then~\eqref{eqn:lift-diagram} commutes.
\end{theorem}
\begin{proof}
Expand definitions and apply homomorphism laws~\eqref{eqn:lift-distributes-over-comp} and~\eqref{eqn:lift-distributes-over-arr} for $\arrowlift\genb$:
\begin{displaybreaks}
\begin{align*}
\numberthis
	\arrowlift\gend~(\arrowtrans\genc~f)
	&\ \equiv\ \fun{j}{\arrowlift\genb~(\arrowarr\gen~snd~\arrowcomp\gen~f)}
\\*
	&\ \equiv\ \fun{j}{\arrowlift\genb~(\arrowarr\gen~snd)~\arrowcomp\genb~\arrowlift\genb~f}
\\
	&\ \equiv\ \fun{j}{\arrowarr\genb~snd~\arrowcomp\genb~\arrowlift\genb~f}
\\*
	&\ \equiv\ \arrowtrans\gend~(\arrowlift\genb~f)
\\[-2.4\baselineskip]
\end{align*}
\end{displaybreaks}
\qedhere
\end{proof}

\begin{theorem}[effectful $AStore$ arrow correctness]
If $\arrowlift\genb$ is an arrow homomorphism from $a$ to $b$, then $\arrowlift\gend$ is an arrow homomorphism from $a^*$ to $b^*$.
\end{theorem}
\begin{proof}
For each homomorphism property~\eqref{eqn:lift-distributes-over-arr}--\eqref{eqn:lift-distributes-over-lazy}, expand the definitions of $\arrowlift\gend$ and the combinator, distribute $\arrowlift\genb$, rewrite in terms of $\arrowlift\gend$, and rewrite using the definition of the combinator.
For example, for distribution over pairing:
\begin{displaybreaks}
\begin{align*}
\numberthis
	\arrowlift\gend~(k_1~\arrowpair\genc~k_2)~j
	&\ \equiv\ \arrowlift\genb~((k_1~\arrowpair\genc~k_2)~j)
\\
	&\ \equiv\ \arrowlift\genb~(k_1~(left~j)~\arrowpair\gen~k_2~(right~j))
\\
	&\ \equiv\ \arrowlift\genb~(k_1~(left~j))~\arrowpair\genb~\arrowlift\genb~(k_2~(right~j))
\\
	&\ \equiv\ (\arrowlift\gend~k_1)~(left~j)~\arrowpair\genb~(\arrowlift\gend~k_2)~(right~j)
\\*
	&\ \equiv\ (\arrowlift\gend~k_1~\arrowpair\gend~\arrowlift\gend~k_2)~j
\end{align*}
\end{displaybreaks}
The remaining properties are similar, though distributing $\arrowlift\gend$ over $\arrowlazy\genc$ requires defining an extra thunk in the last step.
\end{proof}

\begin{corollary}[effectful semantic correctness]
\label{cor:astore-semantic-correctness}
If $\arrowlift\genb$ is an arrow homomorphism, then for all expressions $\mathit{e}$, $\meaningof{\mathit{e}}\gend \equiv \arrowlift\gend~\meaningof{\mathit{e}}\genc$ and $\meaningofconv{\mathit{e}}\gend \equiv \arrowlift\gend~\meaningofconv{\mathit{e}}\genc$.%
\end{corollary}

\begin{corollary}[mapping* and preimage* arrow correctness]
The following diagram commutes:
\begin{equation}
\begin{CD}
X \botto Y   @>\liftmap>>   X \mapto Y   @>\liftpre>>   X \preto Y \\
@V{\eta_\pbot}VV             @VV{\eta\pmap}V              @VV{\eta\ppre}V\\
X \pbotto Y  @>>\liftpmap>  X \pmapto Y  @>>\liftppre>  X \ppreto Y
\end{CD}
\end{equation}
Further, $\liftpmap$ and $\liftppre$ are arrow homomorphisms.
\end{corollary}

As with the correctness of interpretations using the mapping and preimage arrows, the correctness of interpretations using the mapping* and preimage* arrows follows from $\liftpmap$ and $\liftppre$ being arrow homomorphisms, and Theorem~\ref{thm:homomorphism-implies-correct}.

\begin{corollary}[effectful semantic correctness]
For all expressions $\mathit{e}$,
\begin{equation}
\begin{aligned}
	\meaningof{\mathit{e}}\ppre &\ \equiv \ \liftppre~(\liftpmap~\meaningof{\mathit{e}}_\pbot)
\\
	\meaningofconv{\mathit{e}}\ppre &\ \equiv \ \liftppre~(\liftpmap~\meaningofconv{\mathit{e}}_\pbot)
\end{aligned}
\end{equation}
\end{corollary}

Unfortunately, because a statement such as ``$k_1 \equiv k_2$'' implies $k_1$ terminates if and only if $k_2$ terminates, we cannot use the same tactics to prove an asymmetric statement such as ``$k_2$ terminates with the correct answer whenever $k_1$ terminates; otherwise returns $\bot$.''
For these kinds of termination theorems, we need to reason about the interaction of programs with their supplied branch traces.


\subsection{Termination}

Here, we relate $\meaningofconv{\mathit{e}}\genc$ computations, which are interpreted using $\arrowconvif\genc$ and should always terminate, with $\meaningof{\mathit{e}}\genc$ computations, which are interpreted using $\arrowif\genc$ and may not terminate.
To do so, we need to find the largest domain on which $\meaningofconv{\mathit{e}}\genc$ and $\meaningof{\mathit{e}}\genc$ should agree.

\begin{definition}[maximal domain]
\label{def:maximal-domain}
A computation's \mykeyword{maximal domain} is the largest $A^*$ for which
\begin{itemize}
	\item For $f : X \botto Y$, $domain_\bot~f~A^* = A^*$.
	\item For $g : X \mapto Y$, $domain~(g~A^*) = A^*$.
	\item For $h : X \preto Y$, $domain\pre~(h~A^*) = A^*$.
\end{itemize}
The maximal domain of $k : X \arrow\genc Y$ is that of $k~j_0$.
\end{definition}

Because the above statements imply termination, $A^*$ is a subset of the largest domain for which the computations terminate.
It is not too hard to show (but is a bit tedious) that lifting computations preserves the maximal domain; e.g. the maximal domain of $\liftmap~f$ is the same as $f$'s, and the maximal domain of $\liftppre~g$ is the same as $g$'s.

To ensure maximal domains exist, we need the domain operations above to have certain properties.
For the mapping arrow, we must first make the intuition that computations ``act as if they return restricted mappings'' more precise.
First, mapping restriction is defined by
\begin{equation}
\begin{aligned}
	&restrict : (X \pto Y) \tto Set~X \tto (X \pto Y) \\
	&restrict~g~A \ := \ \fun{a \in (A \i domain~g)}{g~a}
\end{aligned}
\end{equation}

\begin{theorem}[mapping arrow restriction]
\label{thm:mapping-arrow-restriction}
Let $g : X \mapto Y$, and $A\conv \subseteq X$ be the largest for which $g~A\conv$ terminates.
For all $A \subseteq A\conv$, $g~A = restrict~(g~A\conv)~A$.%
\end{theorem}
\begin{proof}
By the mapping arrow law (Definition~\ref{def:mapping-arrow-law}) there is an $f : X \botto Y$ such that $g \equiv \liftmap~f$.
Thus,
\begin{displaybreaks}
\begin{align*}
\numberthis
	restrict~(g~A\conv)~A
	&\ \equiv\ restrict~(\liftmap~f~A\conv)~A
\\*
	&\ \equiv\ restrict~(\setb{\pair{a,b} \in mapping~f~A\conv}{b \neq \bot})~A
\\
	&\ \equiv\ \setb{\pair{a,b} \in mapping~f~A}{b \neq \bot}
\\*
	&\ \equiv\ \liftmap~f~A
\\
	&\ \equiv\ g~A
\\[-2.4\baselineskip]
\end{align*}
\end{displaybreaks}
\qedhere
\end{proof}

\begin{theorem}[domain closure operators]
\label{thm:domain-closure-operators}
If $f : X \botto Y$, $g : X \mapto Y$ and $h : X \preto Y$, then $domain_\bot~f$, $domain~{\circ}~g$, and $domain\pre~{\circ}~h$ are monotone, decreasing, and idempotent in the subdomains on which they terminate.%
\end{theorem}
\begin{proof}
These properties follow from the same properties of selection, restriction, and of preimages of images.
\end{proof}

Now we can relate $\meaningofconv{\mathit{e}}_\pbot$ computations to $\meaningof{\mathit{e}}_\pbot$ computations.
First, for any input for which $\meaningof{\mathit{e}}_\pbot$ terminates, there should be a branch trace for which $\meaningofconv{\mathit{e}}_\pbot$ returns the correct output; it should otherwise return $\bot$.

\begin{theorem}
Let $f := \meaningof{\mathit{e}}_\pbot : X \pbotto Y$ with maximal domain $A^*$, and $f' := \meaningofconv{\mathit{e}}_\pbot$.
For all $\pair{\pair{\omega,t},a} \in A^*$, there exists a $T' \subseteq T$ such that
\begin{itemize}
	\item If $t' \in T'$ then $f'~j_0~\pair{\pair{\omega,t'},a} = f~j_0~\pair{\pair{\omega,t},a}$.
	\item If $t' \in T \w T'$ then $f'~j_0~\pair{\pair{\omega,t'},a} = \bot$.
\end{itemize}
\end{theorem}
\begin{proof}
Define $T'$ as the set of all $t' \in J \to Bool_\bot$ such that $t'~j = z$ if the subcomputation with index $j$ is an $if$ whose test returns $z$.
Because $f~j_0~\pair{\pair{\omega,t},a}$ terminates, $t'~j \neq \bot$ for at most finitely many $j$, so each $t' \in T$.

Let $t' \in T'$.
Because the test of every $if$ subcomputation at index $j$ agrees with $t'~j$ and $f$ ignores branch traces, $f'~j_0~\pair{\pair{\omega,t'},a} = f~j_0~\pair{\pair{\omega,t},a}$.

Let $t' \in T \w T'$.
There exists an $if$ subexpression with a test that does not agree with $t'$; therefore $f'~j_0~\pair{\pair{\omega,t'},a} = \bot$.
\end{proof}

Next, for any input for which $\meaningof{\mathit{e}}_\pbot$ does not terminate or returns $\bot$, $\meaningofconv{\mathit{e}}_\pbot$ should return $\bot$.
Proving this is a little easier if we first identify subsets of $J$ that correspond with finite prefixes of an infinite binary tree.

\begin{definition}[index prefix/suffix]
\label{def:index-prefix}
A finite $J' \subset J$ is an \mykeyword{index prefix} if $J' = \set{j_0}$ or, for some index prefix $J''$ and $j \in J''$, $J' = J'' \uplus \set{left~j}$ or $J' = J'' \uplus \set{right~j}$.
The corresponding \mykeyword{index suffix} is $J \w J'$.
\end{definition}

It is not hard to show that every index suffix is closed under $left$ and $right$.

For a given $t \in T$, an index prefix $J'$ serves as a convenient bounding set for the finitely many indexes $j$ for which $t~j \neq \bot$.
Applying $left$ and/or $right$ repeatedly to any $j \in J'$ eventually yields a $j' \in J \w J'$, for which $t~j' = \bot$.

\begin{theorem}
Let $f := \meaningof{\mathit{e}}_\pbot : X \pbotto Y$ with maximal domain $A^*$, and $f' := \meaningofconv{\mathit{e}}_\pbot$.
For all $a \in ((\Omega \times T) \times X) \w A^*$, $f'~j_0~a = \bot$.
\end{theorem}
\begin{proof}
Let $t := snd~(fst~a)$ be the branch trace element of $a$.

Suppose $f~j_0~a$ terminates.
If an $if$ subcomputation's test does not agree with $t$, then $f'~j_0~a = \bot$.
If every $if$'s test agrees, $f'~j_0~a = f~j_0~a = \bot$.

Suppose $f~j_0~a$ does not terminate.
The set of all indexes $j$ for which $t~j \neq \bot$ is contained within an index prefix $J'$.
By hypothesis, there is an $if$ subcomputation at some index $j'$ such that $j' \in J \w J'$.
Because $t~j' = \bot$, $f'~j_0~a = \bot$.
\end{proof}

\begin{corollary}
For all $\mathit{e}$, the maximal domain of $\meaningofconv{\mathit{e}}_\pbot$ is a subset of that of $\meaningof{\mathit{e}}_\pbot$.
\end{corollary}

\begin{corollary}
Let $f' := \meaningofconv{\mathit{e}}_\pbot : X \pbotto Y$ with maximal domain $A^*$, and $f := \meaningof{\mathit{e}}_\pbot$.
For all $a \in A^*$, $f'~j_0~a = f~j_0~a$.
\end{corollary}


\begin{corollary}[correct computation everywhere]
\label{cor:correct-convergence}
Let $\meaningofconv{\mathit{e}}_\pbot : X \pbotto Y$ have maximal domain $A^*$, and $X' := (\Omega \times T) \times X$.
For all $a \in X'$, $A \subseteq X'$ and $B \subseteq Y$,
\begin{equation}
\begin{aligned}
	&\meaningofconv{\mathit{e}}_\pbot &&\!\!\!\!j_0~a &&\!\!\!\!= \ if~(a \in A^*)~(\meaningof{\mathit{e}}_\pbot~j_0~a)~\bot \\
	&\meaningofconv{\mathit{e}}\pmap &&\!\!\!\!j_0~A &&\!\!\!\!= \ \meaningof{\mathit{e}}\pmap~j_0~(A \i A^*) \\
	ap\pre~(\!&\meaningofconv{\mathit{e}}\ppre &&\!\!\!\!j_0~A)~B &&\!\!\!\!= \ ap\pre~(\meaningof{\mathit{e}}\ppre~j_0~(A \i A^*))~B
\end{aligned}
\end{equation}
\end{corollary}

In other words, preimages computed using $\meaningofconv{\cdot}\ppre$ always terminate, never include inputs that give rise to errors or nontermination, and are correct.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Output Probabilities and Measurability}
\label{sec:measurability}

Typically, for $g : \Omega \pto Y$, the probability of $B \subseteq Y$ is $P~(preimage~g~B)$, where $P : Set~\Omega \pto [0,1]$ assigns probabilities to subsets of $\Omega$.

A mapping* computation's domain is $(\Omega \times T) \times X$, not $\Omega$.
We assume each $\omega \in \Omega$ is randomly chosen, but not each $t \in T$ nor each $x \in X$; therefore, neither $T$ nor $X$ should affect the probabilities of output sets.
We clearly must measure \emph{projections} of preimage sets, or $P~(image~(fst~\arrowcomp~fst)~A)$ for preimage sets $A \subseteq (\Omega \times T) \times X$. 

Not all preimage sets have sensible measures.
Sets that do are called \keyword{measurable}.
Computing preimages and projecting them onto $\Omega$ must preserve measurability.

Our main results are the best we could hope for.
First, the interpretations of all expressions are measurable, regardless of nontermination.

\begin{theorem}
\label{thm:proto-all-programs-measurable}
For all expressions $\mathit{e}$, $\meaningofconv{\mathit{e}}\pmap$ is measurable.
\end{theorem}

Second, projecting a program's preimages onto $\Omega$ results in a measurable set.

\begin{theorem}
\label{thm:proto-all-projections-measurable}
If $A \subseteq (\Omega \times T) \times \set{\pair{}}$ is measurable, then $image~(fst~\arrowcomp~fst)~A$ is measurable.
\end{theorem}

The proofs of these theorems are in Chapter~\ref{ch:measurability}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Approximating Semantics}
\label{sec:approximating-semantics}

If we were to confine preimage computation to finite sets, we could implement the preimage arrow directly.
But we would like something that works efficiently on infinite sets, even if it means approximating.
We focus on a specific method: approximating product sets with covering rectangles.

\subsection{Implementable Lifts}

\begin{figure*}[!tb]\centering
\smallmathfont
\begin{align*}
\begin{aligned}[t]
	&\begin{aligned}[t]
		id\pre~A &\ := \ \pair{A,\fun{B}{B}} \\
		const\pre~b~A &\ := \ \pair{\set{b},\fun{B}{if~(B = \emptyset)~\emptyset~A}} \\
		fst\pre~A &\ := \ \pair{proj_1~A,unproj_1~A} \\
		snd\pre~A &\ := \ \pair{proj_2~A,unproj_2~A} \\
		\pi\pre~j~A &\ := \ \pair{proj~j~A, unproj~j~A}
	\end{aligned} \\
\\[-6pt]
\hline
\\[-6pt]
	&\begin{aligned}[t]
		&proj : J \tto Set~(J \to X) \tto Set~X \\
		&proj~j~A \ := \ image~(\pi~j)~A
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&unproj : J \tto Set~(J \to X) \tto Set~X \tto Set~(J \to X) \\
		&unproj~j~A~B \lzfcsplit{
			&\ :=\ preimage~(mapping~(\pi~j)~A)~B \\[2pt]
			&\ \;\equiv\ A \i \prod_{i \in J} if~(j = i)~B~(proj~j~A)
			%&\ :=\ A \i \prod_{j' \in J} if~(j' = j)~B~(proj~j'~A)
		}
	\end{aligned}
\end{aligned}
&\tab\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&proj_1 : Set~\pair{X_1,X_2} \tto Set~X_1 \\
		&proj_1 \ :=\  image~fst \\
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&proj_2 : Set~\pair{X_1,X_2} \tto Set~X_2 \\
		&proj_2 \ :=\  image~snd
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&unproj_1 : Set~\pair{X_1,X_2} \tto Set~X_1 \tto Set~\pair{X_1,X_2} \\
		&unproj_1~A~A_1 \ \lzfcsplit{
			&:=\ preimage~(mapping~fst~A)~A_1 \\[2pt]
			&\;\equiv\ A \i (A_1 \times proj_2~A)
			%&\ :=\ A \i \prod_{j' \in J} if~(j' = j)~B~(proj~j'~A)
		}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&unproj_2 : Set~\pair{X_1,X_2} \tto Set~X_2 \tto Set~\pair{X_1,X_2} \\
		&unproj_2~A~A_2 \ \lzfcsplit{
			&:=\ preimage~(mapping~snd~A)~A_2 \\[2pt]
			&\;\equiv\ A \i (proj_1~A \times A_2)
			%&\ :=\ A \i \prod_{j' \in J} if~(j' = j)~B~(proj~j'~A)
		}
	\end{aligned}
\end{aligned}
\end{align*}
\bottomhrule
\caption[Specific preimage arrow lifts]{Preimage arrow lifts needed to interpret probabilistic programs.}
\label{fig:extra-preimage-arrow-defs}
\end{figure*}

We would like to be able to compute preimages of uncountable sets, such as real intervals---but $preimage~(g~A)~B$ is uncomputable for most mappings $g$ and uncountable sets $A$ and $B$ no matter how cleverly they are represented.
Further, because $pre$, $\liftpre$ and $\arrpre$ are ultimately defined in terms of $preimage$, we cannot implement them.

Fortunately, we need to apply $\arrpre$ only to certain functions.
\figref{fig:semantic-function} (which defines $\meaningof{\cdot}\gen$) lifts $id$, $const~b$, $fst$ and $snd$.
Section~\ref{sec:probabilistic-programs}, which defines the combinators used to interpret partial, probabilistic programs, lifts $\pi~j$ and $agrees$.
Measurable functions made available as language primitives, such as arithmetic, must be lifted to the preimage arrow---though to maintain generality, we put off lifting arithmetic functions until Chapter~\ref{ch:preimage2}.

\figref{fig:extra-preimage-arrow-defs} gives explicit definitions for $\arrpre~id$, $\arrpre~fst$, $\arrpre~snd$, $\arrpre~(const~b)$ and $\arrpre~(\pi~j)$.
(We will deal with $agrees$ separately.)
To implement them, we must model sets in a way that makes $A = \emptyset$ is decidable, and the following representable and finitely computable:
\begin{equation}
\parbox{0.91\textwidth}{
\begin{itemize}
	\item $A \i B$, $\emptyset$, $\set{true}$, $\set{false}$ and $\set{b}$ for every $const~b$
	\item $A_1 \times A_2$, $proj_1~A$ and $proj_2~A$
	\item $J \to X$, $proj~j~A$ and $unproj~j~A~B$
\end{itemize}
}
\label{eqn:exact-rectangle-ops}
\end{equation}
Before addressing representation and computability, we need to define families of sets under which these operations are closed.

\begin{definition}[rectangular family]
\label{def:standard-rectangle}
$Rect~X$ denotes the \mykeyword{rectangular family} of subsets of $X$.
$Rect~X$ must contain $\emptyset$ and $X$, and be closed under finite intersections.
Products must satisfy the following rules:
\begin{align}
	Rect~\pair{X_1,X_2} &\ = \ (Rect~X_1) \boxtimes (Rect~X_2)
	\label{eqn:standard-rect-finite-product-rule}
\\
	Rect~(J \to X) &\ = \ (Rect~X)^{\boxtimes J}
	\label{eqn:standard-rect-arbitrary-product-rule}
%\\
%	X' \in Rect~X \ \implies \ Rect~X' &\ =\ \setb{X' \i A}{A \in Rect~X}
%	\label{eqn:standard-rect-subset-rule}
\end{align}
where the following operations lift cartesian products to sets of sets:
\begin{align}
	\A_1 \boxtimes \A_2 &\ := \ \setb{A_1 \times A_2}{A_1 \in \A_1, A_2 \in \A_2}
\\
	\A^{\boxtimes J} &\ := \ \!\!\!\!\!\!\!\displaystyle\bigcup_{J' \subset J \text{ finite}}\!\!\!\!\!\! \left\{\textstyle\prod_{j \in J} A_j\ \middle|\ A_j \in \A, j \in J' \iff A_j \subset \U\A\right\}
\label{eqn:rectangular-product}
\end{align}
\end{definition}

We additionally define $Rect~Bool ::= \powerset~Bool$.
It is easy to show the collection of all rectangular families is closed under products, projections, and $unproj$.

\begin{figure*}[tb!]\centering
\includegraphics[width=4.5in]{figures/random-tree-rect}
\caption[A finite model of a rectangular subset of $\Omega$]{A finite binary tree model of $unproj~(left~j_0)~(unproj~j_0~\Omega~[0,\frac{1}{4}))~(\frac{3}{4},1]$. Because of $\Omega$'s self-similarity, and because rectangles of $J \to [0,1]$ are defined so that only finitely many projections are not $[0,1]$, every rectangular subset of $\Omega$ has a finite binary tree model.}
\label{fig:omega-rect-tree}
\end{figure*}

Further, all of the operations in~\eqref{eqn:exact-rectangle-ops} can be exactly implemented if finite sets are modeled directly, sets in ordered spaces (such as $\Re$) are modeled by intervals, and sets in $Rect~\pair{X_1,X_2}$ are modeled by pairs of type $\pair{Rect~X_1,Rect~X_2}$.
By~\eqref{eqn:rectangular-product}, sets in $Rect~(J \to X)$ have no more than finitely many projections that are proper subsets of $X$.
They can be modeled by \emph{finite} binary trees, where unrepresented projections are implicitly $X$. \figref{fig:omega-rect-tree} illustrates a model of a member of $Rect~(J \to [0,1])$; i.e. a rectangular subset of $\Omega$.

The set of branch traces $T$ is nonrectangular, but we can model $T$ subsets by $J \to Bool_\bot$ rectangles, implicitly intersected with $T$.

\begin{theorem}[$T$ model]
If $T' \in Rect~(J \to Bool_\bot)$ and $j \in J$, then $proj~j~T' = proj~j~(T' \i T)$.
If $B \subseteq Bool_\bot$, then $unproj~j~(T' \i T)~B = unproj~j~T'~B \i T$.
\end{theorem}
\begin{proof}
Subset case is by projection monotonicity.
For superset, let $b \in proj~j~T'$.
Define $t$ by $t~j' = b$ if $j' = j$; $t~j' = \bot$ if $\bot \in proj~j'~T'$; otherwise $t~j' \in proj~j'~T'$.

By construction, $t \in T'$.
For no more than finitely many $j' \in J$, $t~j' \neq \bot$, so $t \in T$.
Thus, there exists a $t \in T' \i T$ such that $t~j = b$, so $b \in proj~j~(T' \i T)$.

The statement about $unproj$ is an easy corollary.
\end{proof}

\subsection{Approximate Preimage Mapping Operations}

Implementing $\lazypre$ (defined in \figref{fig:preimage-arrow-defs}) requires computing $pre$, but only for the empty mapping, which is trivial: $pre~\emptyset \equiv \pair{\emptyset,\fun{B}\emptyset}$.
Implementing the other combinators requires $(\circ\pre)$, $\pair{\cdot,\cdot}\pre$ and $(\uplus\pre)$.

From the preimage mapping definitions (\figref{fig:preimage-mapping-defs}), we see that $ap\pre$ is defined using $(\i)$ and that $(\circ\pre)$ is defined using $ap\pre$, so $(\circ\pre)$ is directly implementable.
Unfortunately, we hit a snag with $\pair{\cdot,\cdot}\pre$: it loops over possibly uncountably many members of $B$ in a big union.
At this point, we need to approximate.

\begin{theorem}[pair preimage approximation]
\label{thm:pair-preimage-approximation}
Let $g_1 : X \pto Y_1$ and $g_2 : X \pto Y_2$.
For all $B \subseteq Y_1 \times Y_2$, $preimage~\pair{g_1,g_2}\map~B \subseteq preimage~g_1~(proj_1~B) \i preimage~g_2~(proj_2~B)$.%
\end{theorem}
\begin{proof}
By monotonicity of preimages and projections, and by Lemma~\ref{lem:preimage-under-pairing}.
\end{proof}

It is not hard to use Theorem~\ref{thm:pair-preimage-approximation} to show that
\begin{equation}
\begin{aligned}
	&\pair{\cdot,\cdot}\pre' : (X \prepto Y_1) \tto (X \prepto Y_2) \tto (X \prepto Y_1 \times Y_2) \\
	&\pair{\pair{Y_1',p_1},\pair{Y_2',p_2}}\pre' \ := \
		\pair{Y_1' \times Y_2',\fun{B}{p_1~(proj_1~B) \i p_2~(proj_2~B)}}
\end{aligned}
\end{equation}
computes covering rectangles of preimages under pairing.

For $(\uplus\pre)$, we need an approximating replacement for $(\u)$ under which rectangular families are closed.
In other words, we need a lattice join $(\join)$ with respect to $(\subseteq)$, with the following additional properties:
\begin{equation}
\begin{aligned}
	(A_1 \times A_2) \join (B_1 \times B_2) &\ = \ (A_1 \join B_1) \times (A_2 \join B_2) \\
	(\textstyle\prod_{j \in J} A_j) \join (\textstyle\prod_{j \in J} B_j) &\ = \ \textstyle\prod_{j \in J} A_j \join B_j
\label{eqn:join-laws}
\end{aligned}
\end{equation}
If for every nonproduct type $X$, $Rect~X$ is closed under $(\join)$, then rectangular families are clearly closed under $(\join)$. Further, for any $A$ and $B$, $A \u B \subseteq A \join B$.

Replacing each union in $(\uplus\pre)$ with join yields the overapproximating $(\uplus\pre')$:
\begin{equation}
\begin{aligned}
	&(\uplus\pre') : (X \prepto Y) \tto (X \prepto Y) \tto (X \prepto Y) \\
	&\lzfcsplit{
		&h_1 \uplus\pre' h_2 \ := \ 
		\lzfclet{
				Y' & range\pre~h_1 \join range\pre~h_2 \\
				p & \fun{B}{ap\pre~h_1~B \join ap\pre~h_2~B}
			}{\pair{Y',p}}
	}
\end{aligned}
\end{equation}

To interpret programs that may not terminate, or that terminate with probability $1$, we need to approximate $\convifppre$~\eqref{eqn:ifppre-def}, which is defined in terms of $agrees$.
Defining its approximation in terms of an approximation of $agrees$ would not allow us to preserve the fact that expressions interpreted using $\convifppre$ always terminate.
The best approximation of the preimage of $Bool$ under $agrees$ (as a mapping) is $Bool \times Bool$, which contains $\pair{true,false}$ and $\pair{false,true}$, and thus would not constrain the test to agree with the branch trace.

A lengthy (elided) sequence of substitutions to the defining expression for $\convifppre$ results in an $agrees$-free equivalence:
\begin{equation}
\begin{aligned}
	\convifppre~k_1~k_2~k_3~j~A\ \equiv 
	\ \lzfclet{
		\pair{C_k,p_k} & k_1~j_1~A \\
		\pair{C_b,p_b} & branch\ppre~j~A \\
		C_2 & C_k \i C_b \i \set{true} \\
		C_3 & C_k \i C_b \i \set{false} \\
		A_2 & p_k~C_2 \i p_b~C_2 \\
		A_3 & p_k~C_3 \i p_b~C_3 \\
	}{k_2~j_2~A_2 \uplus\pre k_3~j_3~A_3}
\end{aligned}
\label{eqn:expanded-convifppre}
\end{equation}
where $j_1 = left~j$ and so on.
Unfortunately, a straightforward approximation of this would still take unnecessary branches, when $A_2$ or $A_3$ overapproximates $\emptyset$.

$C_b$ is the branch trace projection at $j$ (with $\bot$ removed).
The set of indexes for which $C_b$ is either $\set{true}$ or $\set{false}$ is finite, so it is bounded by an index prefix, outside of which branch trace projections are $\set{true,false}$.
Therefore, if the approximating ${\arrowconvif}\ppre'$ takes \emph{no branches} when $C_b = \set{true,false}$, but approximates with a finite computation, expressions interpreted using ${\arrowconvif}\ppre'$ will always terminate.

We need an overapproximation for the non-branching case.
In the exact semantics, the returned preimage mapping's range is a subset of $Y$, and it returns subsets of $A_2 \uplus A_3$.
Therefore, ${\arrowconvif}\ppre'$ may return $\pair{Y,\fun{B}\lzfcsplit{A_2 \join A_3}}$ when $C_b = \set{true,false}$.
We cannot refer to the type $Y$ in the function definition, so we represent it using $\top$ in the approximating semantics.
Implementations can model it by a singleton ``universe'' instance for every $Rect~Y$.

\begin{figure*}[!tb]\centering
\smallmathfont
\subfloat[Definitions for preimage mappings that compute rectangular covers.]{
\begin{minipage}{0.98\textwidth}
\begin{align*}
\!\!\begin{aligned}[t]
	&\begin{aligned}[t]
		&X \prepto' Y ::= \pair{Rect~Y, Rect~Y \tto Rect~X}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\emptyset\pre' \ := \ \pair{\emptyset,\fun{B} \emptyset}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&ap\pre' : (X \prepto' Y) \tto Rect~Y \tto Rect~X \\
		&ap\pre'~\pair{Y',p}~B := p~(B \i Y') 
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\circ\pre') : (Y \prepto' Z) \tto (X \prepto' Y) \tto (X \prepto' Z) \\
		&\pair{Z',p_2} \circ\pre' h_1 := \pair{Z', \fun{C}{ap\pre'~h_1~(p_2~C)}}
	\end{aligned} \\
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\pair{\cdot,\cdot}\pre' : (X \prepto' Y_1) \tto (X \prepto' Y_2) \tto (X \prepto' Y_1 \times Y_2) \\
		&\pair{\pair{Y_1',p_1},\pair{Y_2',p_2}}\pre' \ := \\
		&\tab\pair{Y_1' \times Y_2',\fun{B}{p_1~(proj_1~B) \i p_2~(proj_2~B)}}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\uplus\pre') : (X \prepto' Y) \tto (X \prepto' Y) \tto (X \prepto' Y) \\
		&\pair{Y_1',p_1} \uplus\pre' \pair{Y_2',p_2} \ := \\
		&\tab\pair{Y_1' \join Y_2',\fun{B}{ap\pre'~\pair{Y_1',p_1}~B \join ap\pre'~\pair{Y_2',p_2}~B}
		}
	\end{aligned}
\end{aligned}
\end{align*}
\vspace{3pt}
\hrule
\end{minipage}
\label{fig:approximating-preimage-mapping-defs}
}

\subfloat[Approximating preimage arrow, defined using approximating preimage mappings.]{
\begin{minipage}{0.98\textwidth}
\begin{align*}
\\[-6pt]
\begin{aligned}[t]
	&\begin{aligned}[t]
		&X \preto' Y ::= Rect~X \tto (X \prepto' Y)
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\comppre') : (X \preto' Y) \tto (Y \preto' Z) \tto (X \preto' Z) \\
		&(h_1~\comppre'~h_2)~A \ := \ 
			\lzfclet{
				h_1' & h_1~A \\
				h_2' & h_2~(range\pre'~h_1')
			}{h_2' \circ\pre' h_1'}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&(\pairpre') : (X \preto' Y_1) \tto (X \preto' Y_2) \tto (X \preto' \pair{Y_1,Y_2}) \\
		&(h_1~\pairpre'~h_2)~A \ := \ \pair{h_1~A,h_2~A}\pre'
	\end{aligned}
\end{aligned}
&\tab\tab\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&\ifpre' : (X \preto' Bool) \tto (X \preto' Y) \tto (X \preto' Y) \tto (X \preto' Y) \\
		&\ifpre'~h_1~h_2~h_3~A \ := \ 
			\lzfclet{
				h_1' & h_1~A \\
				h_2' & h_2~(ap\pre'~h_1'~\set{true}) \\
				h_3' & h_3~(ap\pre'~h_1'~\set{false})
			}{h_2' \uplus\pre' h_3'}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&\lazypre' : (1 \tto (X \preto' Y)) \tto (X \preto' Y) \\
		&\lazypre'~h~A \ := \ if~(A = \emptyset)~\emptyset\pre'~(h~0~A)
	\end{aligned}
\end{aligned}
\end{align*}
\vspace{3pt}
\hrule
\end{minipage}
\label{fig:approximating-preimage-arrow-defs}
}

\subfloat[Preimage* arrow combinators for probabilistic choice and guaranteed termination.
\figref{fig:astore-arrow-defs}~($AStore$ arrow transformer) defines $\arrowtrans\ppre'$, $(\compppre')$, $(\pairppre')$, $\ifppre'$ and $\lazyppre'$.]{
\begin{minipage}{0.98\textwidth}
\begin{align*}
\\[-6pt]
\!\!\!\begin{aligned}[t]
	&\begin{aligned}[t]
		&X \ppreto' Y ::= AStore~(\Omega \times T)~(X \preto' Y)
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&random\ppre' : X \ppreto' [0,1] \\
		&\lzfcsplit{&random\ppre'~j \ := \ \\ &\tab fst\pre~\comppre'~fst\pre~\comppre'~\pi\pre~j}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		&branch\ppre' : X \ppreto' Bool \\
		&\lzfcsplit{&branch\ppre'~j \ := \ \\ &\tab fst\pre~\comppre'~snd\pre~\comppre'~\pi\pre~j}
	\end{aligned} \\
\\[-6pt]
	&\begin{aligned}[t]
		fst\ppre' &:= \arrowtrans\ppre'~fst\pre;\ \cdots
	\end{aligned}
\end{aligned}
&\tab
\begin{aligned}[t]
	&\begin{aligned}[t]
		&{\arrowconvif}\ppre' : (X \ppreto' Bool) \tto (X \ppreto' Y) \tto (X \ppreto' Y) \tto (X \ppreto' Y) \!\!\!\!\!\!\!\!\!\\
		&\lzfcsplit{
			&{\arrowconvif}\ppre'~k_1~k_2~k_3~j \ := \\
			&\tab\lzfclet{
				\pair{C_k,p_k} & k_1~(left~j)~A \\
				\pair{C_b,p_b} & branch\ppre~j~A \\
				C_2 & C_k \i C_b \i \set{true} \\
				C_3 & C_k \i C_b \i \set{false} \\
				A_2 & p_k~C_2 \i p_b~C_2 \\
				A_3 & p_k~C_3 \i p_b~C_3 \\
			}{\lzfccase{C_b}
				{
					\set{true,false} & \pair{\top,\fun{B}\lzfcsplit{A_2 \join A_3}} \\
					\set{true} & k_2~(left~(right~j))~A_2 \\
					\set{false} & k_3~(right~(right~j))~A_3
				}
			}
		}
	\end{aligned}
\end{aligned}
\end{align*}
\vspace{3pt}
\hrule
\end{minipage}
\label{fig:approximating-preimage*-arrow-defs}
}
\caption[Implementable, approximating arrows]{Implementable arrows that approximate preimage arrows.
Specific lifts such as $fst\pre := \arrpre~fst$ are computable (see \figref{fig:extra-preimage-arrow-defs}), but $\arrpre'$ is not.
}
\label{fig:approximating-arrow-defs}
\end{figure*}

\figref{fig:approximating-preimage-arrow-defs} defines the final approximating preimage arrow.
This arrow, the lifts in \figref{fig:extra-preimage-arrow-defs}, and the semantic function $\meaningof{\cdot}\gen$ in \figref{fig:semantic-function} define an approximating semantics for partial, probabilistic programs.

\subsection{Correctness}

From here on, ${\meaningofconv{\cdot}}\ppre'$ interprets programs as approximating preimage* arrow computations using ${\arrowconvif}\ppre'$.
The following theorems assume $h := \meaningofconv{\mathit{e}}\ppre : X \ppreto Y$ and $h' := {\meaningofconv{\mathit{e}}}\ppre' : X \ppreto' Y$ for some expression $\mathit{e}$.

To use structural induction on the interpretation of $\mathit{e}$, we need a theorem that allows representing it as a finite expression (Definition~\ref{def:finite-expression}).
Because ${\arrowconvif}\ppre'$ does not branch when either branch could be taken, an equivalent finite expression exists for each rectangular domain subset $A$.

\begin{theorem}[equivalent finite expression]
\label{thm:equivalent-finite-expression}
For all $A \in Rect~\pair{\pair{\Omega,T},X}$,
there exists a finite expression $\mathit{e'}$ for which, if $h'' := {\meaningofconv{\mathit{e'}}}\ppre'$, then $ap\pre'~(h''~j_0~A)~B = ap\pre'~(h'~j_0~A)~B$ for all $B \in Rect~Y$.%
\end{theorem}
\begin{proof}
Let $T' := proj_2~(proj_1~A)$, and let the index prefix $J'$ contain every $j'$ for which $(proj~j'~T') \w \set{\bot}$ is either $\set{true}$ or $\set{false}$.
To construct $\mathit{e'}$, exhaustively apply first-order functions in $\mathit{e}$, but replace any $if~\mathit{e}_1~\mathit{e}_2~\mathit{e}_3$ whose index is not in $J'$ with the equivalent expression $if~\mathit{e}_1~\bot~\bot$.
Because $\mathit{e}$ is well-defined, recurrences must be guarded by $if$, so this process terminates after finitely many applications.
\end{proof}

\begin{corollary}[terminating]
\label{thm:terminating}
For all $A \in Rect~\pair{\pair{\Omega,T},X}$ and $B \in Rect~Y$, $ap\pre'~(h'~j_0~A)~B$ terminates.
\end{corollary}

\begin{theorem}[sound]
\label{thm:approximation}
For all $A \in Rect~\pair{\pair{\Omega,T},X}$ and $B \in Rect~Y$, $ap\pre~(h~j_0~A)~B \subseteq ap\pre'~(h'~j_0~A)~B$.
\end{theorem}
\begin{proof}
By construction and Corollary~\ref{thm:terminating} (recall non-``$\equiv$'' statements imply termination).
\end{proof}


\begin{theorem}[monotone]
\label{thm:monotonicity}
$ap\pre'~(h'~j_0~A)~B$ is monotone in both $A$ and $B$.%
\end{theorem}
\begin{proof}
Lattice operators $(\i)$ and $(\join)$ are monotone, as is $(\times)$.
Therefore, $id\pre$ and the other lifts in \figref{fig:extra-preimage-arrow-defs} are monotone, and each approximating preimage arrow combinator preserves monotonicity.
Approximating preimage* arrow combinators, which are defined in terms of approximating preimage arrow combinators (\figref{fig:approximating-preimage-arrow-defs}) likewise preserve monotonicity, as does $\arrowtrans\ppre'$; therefore $id\ppre$ and other lifts are monotone.

The definition of ${\arrowconvif}\ppre'$ can be written in terms of lattice operators and approximating preimage arrow combinators for any $A$ for which $C_b = \set{true}$ or $C_b = \set{false}$, and thus preserves monotonicity in those cases.
If $C_b = \set{true,false}$, which is an upper bound for $C_b$, the returned value is an upper bound.

For monotonicity in $A$, suppose $A_1 \subseteq A_2$.
Apply Theorem~\ref{thm:equivalent-finite-expression} with $A_1$ to yield $\mathit{e'}$; clearly, it is also an equivalent finite expression for $A_2$.
Monotonicity follows from structural induction on the interpretation of $\mathit{e'}$.

For monotonicity in $B$, apply Theorem~\ref{thm:equivalent-finite-expression} with a fixed $A$.
\end{proof}

\begin{theorem}[decreasing]
\label{thm:decreasing}
If $A \in Rect~\pair{\pair{\Omega,T},X}$ and $B \in Rect~Y$, $ap\pre'~(h'~j_0~A)~B \subseteq A$.%
\end{theorem}
\begin{proof}
Because they compute exact preimages of rectangular sets under restriction to rectangular domains, $id\pre$ and the other lifts in \figref{fig:extra-preimage-arrow-defs} are decreasing.

By definition and applying basic lattice properties,
\begin{displaybreaks}
\begin{align*}
\numberthis
	ap\pre'~((h_1~\comppre'~h_2)~A)~B &\ \equiv\  ap\pre'~(h_1~A)~B'\ \text{ for some $B'$}
\\*
	ap\pre'~((h_1~\pairpre'~h_2)~A)~B &\ \equiv\
		ap\pre'~(h_1~A)~(proj_1~B)~\i~ap\pre'~(h_2~A)~(proj_2~B)
\\
	ap\pre'~(\ifpre'~h_1~h_2~h_3~A)~B &\ \equiv\
		\lzfclet{
			A_2 & ap\pre'~(h_1~A)~\set{true} \\
			A_3 & ap\pre'~(h_1~A)~\set{false}
		}{ap\pre'~(h_2~A_2)~B~\join~ap\pre'~(h_3~A_3)~B}
\\
	ap\pre'~(\lazypre'~h~A)~B &\ \equiv\ if~(A = \emptyset)~\emptyset~(ap\pre'~(h~0~A)~B)
\end{align*}
\end{displaybreaks}
Thus, approximating preimage arrow combinators return decreasing computations when given decreasing computations.
This property transfers trivially to approximating preimage* arrow combinators.
Apply Theorem~\ref{thm:equivalent-finite-expression} and use structural induction.
\end{proof}

\subsection{Preimage Refinement Algorithm}
\label{sec:discretization}

Given these properties, we might try to compute exact preimages of $B$ by computing preimages with respect to increasingly fine discretizations of $A$.

\begin{definition}[preimage refinement algorithm]
\label{def:preimage-refinement}
Let $B \in Rect~Y$ and
\begin{equation}
\begin{aligned}
	&refine : Rect~\pair{\pair{\Omega,T},X} \tto Rect~\pair{\pair{\Omega,T},X} \\
	&refine~A\ :=\ ap\pre'~(h'~j_0~A)~B
\end{aligned}
\end{equation}
Define $split : Rect~\pair{\pair{\Omega,T},X} \tto Set~(Rect~\pair{\pair{\Omega,T},X})$ to produce positive-measure, disjoint rectangles, and define
\begin{equation}
\begin{aligned}
	&refine^* : Set~(Rect~\pair{\pair{\Omega,T},X}) \tto Set~(Rect~\pair{\pair{\Omega,T},X}) \\
	&refine^*~\A\ :=\ image~refine~\left(\U_{A \in \A} split~A \right)
\end{aligned}
\end{equation}
For any positive-measure $A_0 \in Rect~\pair{\pair{\Omega,T},X}$, iterate $refine^*$ on $\set{A_0}$.
\end{definition}

\newcommand{\betweenrefinementfigurehspace}{\hspace{0.07in}}
\newcommand{\subfigurewidth}{2.05in}

\begin{figure*}[tb!]\centering
\subfloat[Exact preimage of $B$]{%
\label{fig:preimage-refinement:exact}%
\includegraphics[width=\subfigurewidth]{figures/refinement/constrained-norm-norm-source-00}%
}%
\betweenrefinementfigurehspace%
\subfloat[Initial partition $\A_0 := \set{A_0}$]{%
\includegraphics[width=\subfigurewidth]{figures/refinement/constrained-norm-norm-source-01}%
}%
\betweenrefinementfigurehspace%
\subfloat[$\A_0' := \U_{A \in \A_0} split~A$]{%
\includegraphics[width=\subfigurewidth]{figures/refinement/constrained-norm-norm-source-02}%
}

\vspace{0.5\baselineskip}

\subfloat[$\A_1 := image~refine~\A_0'$]{%
\includegraphics[width=\subfigurewidth]{figures/refinement/constrained-norm-norm-source-03}%
}%
\betweenrefinementfigurehspace%
\subfloat[$\A_1' := \U_{A \in \A_1} split~A$]{%
\includegraphics[width=\subfigurewidth]{figures/refinement/constrained-norm-norm-source-04}%
}%
\betweenrefinementfigurehspace%
\subfloat[$\A_2 := image~refine~\A_1'$]{%
\includegraphics[width=\subfigurewidth]{figures/refinement/constrained-norm-norm-source-05}%
}

\vspace{\baselineskip}

\subfloat[Further preimage refinements $\A_3 := refine^*~\A_2$, $\A_4 := refine^*~\A_3$ and $\A_5 := refine^*~\A_4$]{%
\includegraphics[width=\subfigurewidth]{figures/refinement/constrained-norm-norm-source-07}%
\betweenrefinementfigurehspace%
\includegraphics[width=\subfigurewidth]{figures/refinement/constrained-norm-norm-source-09}%
\betweenrefinementfigurehspace%
\includegraphics[width=\subfigurewidth]{figures/refinement/constrained-norm-norm-source-11}%
}

\caption[Preimage refinement algorithm]{Preimage refinement algorithm on $\pair{\pair{\Omega,T},X}$.
Only two dimensions of $\Omega$ are shown.
In this example, the covering partition appears to converge in measure to the exact. (In the worst case, \ref{fig:preimage-refinement:exact} represents an open set, which in the limit, preimage refinement overapproximates only on the boundary.)}
\label{fig:preimage-refinement}
\end{figure*}

\figref{fig:preimage-refinement} illustrates the preimage refinement algorithm.

Theorem~\ref{thm:decreasing} (decreasing) guarantees $refine~A$ is never larger than $A$.
Theorem~\ref{thm:monotonicity} (monotone) guarantees refining a \emph{partition} of $A$ never does worse than refining $A$ itself.
Theorem~\ref{thm:approximation} (sound) guarantees the algorithm is sound: the exact preimage of $B$ is always contained in the covering partition $refine^*$ returns.

We would like it to be precise in the limit, up to null sets: covering partitions' measures should converge to the true measure.
Unfortunately, preimage refinement appears to compute the \keyword{Jordan outer measure} of a preimage, which is not always its measure.
A counterexample is the expression $rational?~random$, where $rational?$ returns $true$ when its argument is rational and loops otherwise.
(This is definable using a $(\leq)$ primitive.)
The preimage of $\set{true}$ (the rational numbers) has measure $0$, but its Jordan outer measure is $1$.

We conjecture that a minimal requirement for preimage refinement's measures to converge is that the program must terminate with probability $1$.
There are certainly other requirements.
We leave these and proof of convergence of measures for future work.

For now, we use algorithms that depend only on soundness.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Implementations}
\label{sec:implementation}

We have four implementations: one of the exact semantics, two direct implementations of the approximating semantics, and a less direct but more efficient implementation of the approximating semantics, which we call \mykeyword{Dr. Bayes}.

If sets are restricted to be finite, the arrows used as translation targets in the exact semantics, defined in \ref{fig:bottom-arrow-defs}, \ref{fig:mapping-arrow-defs}, \ref{fig:preimage-mapping-defs}, \ref{fig:preimage-arrow-defs} and~\ref{fig:astore-arrow-defs}, can be implemented directly in any practical $\lambda$-calculus.
Computing exact preimages is very inefficient, even under the interpretations of very small programs.
Still, we have found our Typed Racket~\cite{cit:tobin-hochstadt-2008popl-typed-scheme} implementation useful for finding theorem candidates and counterexamples.

Given a rectangular set library, the approximating preimage arrows defined in \figsref{fig:extra-preimage-arrow-defs} and~\ref{fig:approximating-preimage-arrow-defs} can be implemented with few changes in any practical $\lambda$-calculus.
We have done so in Typed Racket and Haskell~\cite{cit:haskell-lang}.
Both implementations' arrow combinator definitions are almost line-for-line transliterations from the figures.
They are at \url{https://github.com/ntoronto/writing/tree/master/2014esop-code}. [XXX: move]

Making the rectangular set type polymorphic seems to require the equivalent of a typeclass system.
In Haskell, it also requires multi-parameter typeclasses or indexed type families~\cite{cit:chakravarty-2005popl-type-families} to associate set types with the types of their members.
Using indexed type families, the only significant differences between the Haskell implementation and the approximating semantics are type contexts, \texttt{newtype} wrappers for arrow types, and using \texttt{Maybe} types as bottom arrow return types.

Typed Racket has no typeclass system on top of its type system, so the rectangular set type is monomorphic; thus, so are the arrow types.
The lack of type variables in the combinator types is the only significant difference between the implementation and the approximating semantics.

Chapter~\ref{ch:preimage2} details the implementation of Dr. Bayes.

\section{Conclusions}

To allow recursion and arbitrary conditions in probabilistic programs, we combined the power of measure theory with the unifying elegance of arrows. We
\begin{enumerate}
	\item Defined a transformation from first-order programs to arbitrary arrows.
	\item Defined the bottom arrow as a standard translation target.
	\item Derived the uncomputable preimage arrow as an alternative target.
	\item Derived a sound, computable approximation of the preimage arrow, and enough computable lifts to transform programs.
\end{enumerate}
Critically, the preimage arrow's lift from the bottom arrow distributes over bottom arrow computations.
Our semantics thus generalizes this process to all programs: 1) encode a program as a bottom arrow computation; 2) lift this computation to get an uncomputable function that computes preimages; 3) distribute the lift; and 4) replace uncomputable expressions with sound approximations.

Using arrows drastically simplifies the correctness proofs.
Almost every semantic correctness theorem proceeds from a proof that a lift distributes over five combinators.
There are seven theorems in total corresponding to the morphisms in our roadmap~\eqref{eqn:roadmap-diagram1}, but the three center morphisms (pointing downward) are done in one proof, as are the two bottom morphisms (pointing rightward).
In total, there are 20 cases, plus 11 for the original (and very simple) proof by induction that arrow homomorphisms distribute over program terms.

In contrast, the corresponding theorems with separate semantic functions would require seven proofs by structural induction over at least 11 rules (12 for programs that access the random store), for a total of at least 77 cases. This reduction in complexity by semantic abstraction would have been difficult without targeting \lzfclang, which allows such arrows to carry out uncountably infinite computations.

Further, because the approximating semantics targets a computable \lzfclang sublanguage, it is directly implementable.
The next chapter details creating a \emph{practical} implementation.

\mathversion{normal}
